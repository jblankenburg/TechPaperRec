{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = \"/home/janelle/Documents/classes/complexNetworks/paper/texfiles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles\n"
     ]
    }
   ],
   "source": [
    "print base_path\n",
    "files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/150601732.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/171111575.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/170807933.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/171011252.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/170501921.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/170807878.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/171111577.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/170701243.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/170900930.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/171100499.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/171200516.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/170402787.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/160407457.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/161208263.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/08092322.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/171011304.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/171001576.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/170909708.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/171001578.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/08094019.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/170903846.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/08091344.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/08091681.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/171101867.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/08091330.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/08091258.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/08092152.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/171006454.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/170603024.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/171105858.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/151104224.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/170300212.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/170501263.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/170506086.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/170905418.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/171107793.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/170601208.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/14067025.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/170708360.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/171111123.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/171108078.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/171200242.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/17110964.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/170310361.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/171201207.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/171200061.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/171200865.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/171104188.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/170305375.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/171201177.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/171200676.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/171111377.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/171111521.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/1505105613.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/150308294.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/150504956.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/11036006.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/150504636.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/11033782.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/11052279.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/11052301.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/11036087.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/150503340.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/11034204.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/150504694.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/150504542.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/150401365.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/11034550.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/11033532.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/11052447.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/170902098.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/171004490.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/170902102.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/171008647.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/170907171.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/171007528.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/170904380.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/171005061.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/171000213.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/171004115.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/170903122.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/171008748.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/14096226.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170909741.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170302335.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170910489.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/171008612.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170302279.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170302326.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170302640.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170702920.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170907857.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170601127.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170301423.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170302340.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170301040.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/170906223.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/150501245.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/171201139.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/161208678.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/150507370.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/161101907.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/170908025.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/170701871.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/150507487.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/150507929.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/150507548.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/150502385.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/170904905.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/160509221.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/171111139.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/171105914.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/170404689.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/171109091.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/160601990.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/171111585.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/171109874.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/17110456.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/170505035.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/171101744.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/171107676.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/161102779.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/171102771.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/171007309.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/170502730.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/170907092.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/171007308.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/170701271.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/170503667.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/170403105.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/170309264.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/171103016.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/171111396.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/171008332.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/170704245.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/171101845.tex <type 'str'>\n",
      "/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/171000803.tex <type 'str'>\n"
     ]
    }
   ],
   "source": [
    "# get list of all files to parse\n",
    "for (dirpath, dirnames, filenames) in os.walk(base_path):\n",
    "    for f in filenames:\n",
    "        if f.endswith(\".tex\"):\n",
    "#             print(os.path.join(dirpath, f))\n",
    "            temp = os.path.join(dirpath, f)\n",
    "            print temp, type(temp)\n",
    "            files.append(temp)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/150601732.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/171111575.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/170807933.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/171011252.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/170501921.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/170807878.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/171111577.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/170701243.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/170900930.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/171100499.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/171200516.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/170402787.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/160407457.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/161208263.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/08092322.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/171011304.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/171001576.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/170909708.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/171001578.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/08094019.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/170903846.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/08091344.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/08091681.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/171101867.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/08091330.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/08091258.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/08092152.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/networking/tex/171006454.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/170603024.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/171105858.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/151104224.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/170300212.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/170501263.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/170506086.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/170905418.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/171107793.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/170601208.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/14067025.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/170708360.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/graphics/tex/171111123.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/171108078.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/171200242.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/17110964.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/170310361.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/171201207.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/171200061.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/171200865.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/171104188.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/170305375.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/171201177.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/171200676.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/171111377.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/SoftwareEngineering/tex/171111521.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/1505105613.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/150308294.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/150504956.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/11036006.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/150504636.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/11033782.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/11052279.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/11052301.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/11036087.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/150503340.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/11034204.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/150504694.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/150504542.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/150401365.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/11034550.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/11033532.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Parallel/tex/11052447.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/170902098.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/171004490.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/170902102.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/171008647.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/170907171.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/171007528.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/170904380.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/171005061.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/171000213.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/171004115.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/170903122.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/automata/tex/171008748.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/14096226.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170909741.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170302335.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170910489.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/171008612.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170302279.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170302326.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170302640.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170702920.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170907857.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170601127.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170301423.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170302340.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Robotics/tex/170301040.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/170906223.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/150501245.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/171201139.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/161208678.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/150507370.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/161101907.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/170908025.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/170701871.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/150507487.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/150507929.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/150507548.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/Security/tex/150502385.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/170904905.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/160509221.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/171111139.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/171105914.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/170404689.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/171109091.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/160601990.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/171111585.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/171109874.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/17110456.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/170505035.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/171101744.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/171107676.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/161102779.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/ML/tex/171102771.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/171007309.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/170502730.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/170907092.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/171007308.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/170701271.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/170503667.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/170403105.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/170309264.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/171103016.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/171111396.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/171008332.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/170704245.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/171101845.tex', '/home/janelle/Documents/classes/complexNetworks/paper/texfiles/compilers/tex/171000803.tex']\n"
     ]
    }
   ],
   "source": [
    "print files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(files[0], \"r\")\n",
    "# k =  f.read()\n",
    "content = f.readlines()\n",
    "# for f in files:\n",
    "#     doc = open(f,\"r\")\n",
    "#     print doc.read() \n",
    "#     print \"________________________________________________\\n\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/janelle/Documents/classes/complexNetworks/paper/texfiles/CV/tex/150601732.tex'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print k\n",
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\\\RequirePackage{snapshot}\\n', '\\\\documentclass[conference]{IEEEtran}\\n', '\\n', '% Packages\\n', '% -----------------------------------------------------------------------\\n', '\\n', '% \\\\newenvironment{ppl}{\\\\fontfamily{phv}\\\\selectfont}{\\\\par}\\n', '\\\\usepackage{amsfonts,amsmath,amssymb,mathtools}\\n', '\\\\usepackage{flushend}\\n', '\\\\usepackage{bigints}\\n', '\\\\usepackage{hhline}\\n', '\\\\usepackage{marvosym} % Mail symbol\\n', '\\n', '\\n', '% Font\\n', '% -----------------------------------------------------------------------\\n', '% \\\\usepackage{times}\\n', '\\\\usepackage{tgpagella}\\n', '\\\\usepackage[T1]{fontenc}\\n', '\\n', '\\n', '% Graphics\\n', '% -----------------------------------------------------------------------\\n', '\\\\usepackage{graphicx}\\n', '\\\\usepackage{booktabs, multicol, multirow}\\n', '\\\\usepackage{indentfirst}\\n', '\\\\usepackage{subfigure}\\n', '\\\\usepackage[font=footnotesize]{caption}\\n', '\\\\DeclareCaptionLabelFormat{andtable}{#1~#2  \\\\&  \\\\tablename~\\\\thetable}\\n', '\\\\DeclareCaptionLabelFormat{andfigure}{\\\\tablename~\\\\thetable~\\\\&~#1~#2}\\n', '\\n', '% Inlined commenting command\\n', '% -----------------------------------------------------------------------\\n', '\\\\usepackage{marginnote}\\n', '\\\\usepackage[usenames,dvipsnames]{color}\\n', '\\\\usepackage[table]{xcolor}    % loads also \\xc2\\xbbcolortbl\\xc2\\xab\\n', '\\\\setlength{\\\\marginparwidth}{0.5in}\\n', '\\\\definecolor{fullred}{rgb}{0.85,.0,.1} \\\\newcommand{\\\\fr}{\\\\color{fullred}}\\n', '\\\\definecolor{navyblue}{rgb}{.0,.0,.5}\\n', '\\\\definecolor{bleudefrance}{rgb}{0.19, 0.55, 0.91}\\n', '\\\\definecolor{bluegray}{rgb}{0.18, 0.36, 0.6}\\n', '% \\\\newcommand{\\\\todosp}[1]{\\\\marginpar{\\\\tiny\\\\noindent{\\\\raggedright\\\\color{blue}\\n', '%             {[SP]}\\\\fr{ #1}} \\\\par}}\\n', '\\\\newcommand{\\\\todoinsp}[1]{{\\\\noindent{\\\\raggedright\\\\color{blue}\\n', '      {[SP]}\\\\fr{ #1}} \\\\par}}\\n', '\\n', '\\\\newcommand{\\\\todosp}[1]{{\\\\color{bluegray}{#1}}}\\n', '\\n', '\\\\usepackage[textsize=tiny]{todonotes}\\n', '\\n', '% Bold row for tables\\n', '% -----------------------------------------------------------------------\\n', '\\\\usepackage{array}\\n', '\\\\newcolumntype{+}{>{\\\\global\\\\let\\\\currentrowstyle\\\\relax}}\\n', '\\\\newcolumntype{^}{>{\\\\currentrowstyle}}\\n', '\\\\newcommand{\\\\rowstyle}[1]{\\\\gdef\\\\currentrowstyle{#1}%\\n', '#1\\\\ignorespaces\\n', '}\\n', '\\\\newcolumntype{C}[1]{>{\\\\centering\\\\arraybackslash}p{#1}}\\n', '\\n', '% Bib, Multicol, refs\\n', '% numbers option provides compact numerical references in the text. \\n', '% -----------------------------------------------------------------------\\n', '\\\\usepackage[numbers,sort]{natbib}\\n', '\\\\usepackage[bookmarks=true, colorlinks=true, urlcolor=Blue]{hyperref}\\n', '\\\\usepackage{multicol}\\n', '\\n', '\\n', '% Custom defined variables for general purpose needs\\n', '% -----------------------------------------------------------------------\\n', '% UNCOMMENT FOR CAMERA-READY SUBMISSION\\n', '\\\\newcommand{\\\\ME}{Sudeep Pillai}\\n', '\\\\newcommand{\\\\PAPERAUTHORS}{Sudeep Pillai and John J. Leonard}\\n', '\\\\newcommand{\\\\PAPERTITLE}{Monocular SLAM Supported Object Recognition}\\n', '\\\\newcommand{\\\\PAPERTITLEFMT}{Monocular SLAM Supported\\\\\\\\Object Recognition}\\n', '\\\\newcommand{\\\\PAPERKEYWORDS}{Object Recognition; Computer Vision; Perception}\\n', '\\n', '% http://tex.stackexchange.com/questions/48067/pdfinfo-doesnt-appear-to-be-working\\n', \"% hyperref and pdfinfo don't work together, use either\\n\", '\\\\hypersetup{pdfauthor={\\\\ME},%\\n', '            pdftitle={\\\\PAPERTITLE},%\\n', '            pdfsubject={\\\\PAPERTITLE},%\\n', '            pdfkeywords={\\\\PAPERKEYWORDS},%\\n', '            pdfproducer={LaTeX},%\\n', '            pdfcreator={pdfLaTeX}\\n', '}\\n', '\\n', '\\\\setlength{\\\\textfloatsep}{2pt}\\n', '\\n', '% BEGIN\\n', '% -----------------------------------------------------------------------\\n', '\\\\begin{document}\\n', '\\n', '% paper title\\n', '\\\\title{\\\\PAPERTITLEFMT{}\\\\vspace{-0.5ex}}\\n', '\\n', '% \\\\authorrefmark{1}\\n', '\\n', '% UNCOMMENT FOR CAMERA-READY SUBMISSION\\n', '\\\\author{\\\\authorblockN{\\\\PAPERAUTHORS{}}\\n', '\\\\authorblockA{\\n', 'Computer Science and Artificial Intelligence Laboratory\\\\\\\\\\n', 'Massachusetts Institute of Technology\\\\\\\\\\n', '\\\\{\\\\href{mailto:spillai@csail.mit.edu}{spillai}, \\\\href{mailto:jleonard@csail.mit.edu}{jleonard}\\\\}@csail.mit.edu}}\\n', '\\n', '% -----------------------------------------------------------------------\\n', '\\\\maketitle\\n', '\\n', '% ABSTRACT\\n', '% -----------------------------------------------------------------------\\n', '\\\\vspace{-1ex}\\n', '\\\\begin{abstract}\\n', 'In this work, we develop a monocular SLAM-aware object recognition system that\\n', 'is able to achieve considerably stronger recognition performance, as compared to\\n', 'classical object recognition systems that function on a frame-by-frame basis. By\\n', 'incorporating several key ideas including multi-view object proposals and\\n', 'efficient feature encoding methods, our proposed system is able to detect and\\n', 'robustly recognize objects in its environment using a single RGB camera in\\n', 'near-constant time. Through experiments, we illustrate the utility of using such\\n', 'a system to effectively detect and recognize objects, incorporating multiple\\n', 'object viewpoint detections into a unified prediction hypothesis. The\\n', 'performance of the proposed recognition system is evaluated on the UW RGB-D\\n', 'Dataset, showing strong recognition performance and scalable run-time\\n', 'performance compared to current state-of-the-art recognition systems.\\n', '\\n', '\\\\end{abstract}\\n', '\\n', '\\\\IEEEpeerreviewmaketitle\\n', '\\n', '% CONTENT\\n', '% -----------------------------------------------------------------------\\n', '\\\\section{Introduction}\\n', \"\\\\label{sec-introduction} Object recognition is a vital component in a robot's\\n\", 'repertoire of skills. Traditional object recognition methods have focused on\\n', 'improving recognition performance (Precision-Recall, or mean Average-Precision)\\n', 'on specific datasets~\\\\cite{everingham2010pascal, ILSVRC15}. While these datasets\\n', 'provide sufficient variability in object categories and instances, the training\\n', 'data mostly consists of images of arbitrarily picked scenes and/or\\n', 'objects. Robots, on the other hand, perceive their environment as a continuous\\n', 'image stream, observing the same object several times, and from multiple\\n', 'viewpoints, as it constantly moves around in its immediate environment. As a\\n', 'result, object detection and recognition can be further bolstered if the\\n', 'robot were capable of simultaneously localizing itself and mapping (SLAM) its\\n', 'immediate environment - by integrating object detection evidences\\n', 'across multiple views.\\n', '\\n', '% Additionally, robots can observe their immediate surroundings via these\\n', '% streams to localize and map itself quite robustly, presenting a far-more\\n', '% structured data representation compared to arbitrary hand-labeled images of\\n', '% object instances on the web. \\n', '\\n', '\\n', '% A robot would benefit from the ability to identify objects in its immediate\\n', '% surroundings, including those it has observed in its recent past, to act upon\\n', '% them when necessary. \\n', '\\n', '% Robots, however, function in rich and contextual spaces, with the data observed\\n', '% being highly structured. The structured data refers to the concept of observed\\n', '% data being spatially and temporally correlated.  \\n', '\\n', '\\n', '% Intro figure\\n', '%\\n', '\\\\begin{figure}[!t]\\n', '  \\\\centering\\n', '  \\\\begin{tabular}{c}\\n', '    \\\\includegraphics[width=\\\\columnwidth]{graphics/slam_detect/intro.pdf}\\n', '  \\\\end{tabular}\\n', '  \\\\caption{The proposed SLAM-aware object recognition system is able to robustly\\n', '    localize and recognize several objects in the scene, aggregating detection evidence across multiple views. Annotations in white are provided for clarity and are actual predictions\\n', '    proposed by our system.}\\n', '  \\\\label{fig:intro-fig}\\n', '\\\\end{figure}\\n', '\\n', '\\n', '\\n', '% \\\\textit{Using an RGB camera as the only input, we propose a SLAM-aware\\n', '% object recognition system that is considerably stronger in performance than a\\n', '% classical recognition system that detects objects on a per-frame basis}. \\n', '\\n', \"We refer to a ``SLAM-aware'' system as - one that has access to the map of its\\n\", 'observable surroundings as it builds it incrementally and the location of its\\n', 'camera at any point in time. This is in contrast to classical recognition\\n', \"systems that are ``SLAM-oblivious'' - those that detect and recognize objects on\\n\", 'a frame-by-frame basis without being cognizant of the map of its environment, the\\n', 'location of its camera, or that objects may be situated within these\\n', 'maps. \\\\textit{In this paper, we develop the ability for a SLAM-aware system to\\n', 'robustly recognize objects in its environment, using an RGB camera as its only\\n', 'sensory input} (Figure~\\\\ref{fig:intro-fig}).\\n', '\\n', 'We make the following contributions towards this end: Using state-of-the-art\\n', 'semi-dense map reconstruction techniques in monocular visual SLAM as\\n', 'pre-processed input, we introduce the capability to propose multi-view\\n', 'consistent object candidates, as the camera observes instances of objects across\\n', 'several disparate viewpoints. Leveraging this object proposal method, we\\n', 'incorporate some of the recent advancements in bag-of-visual-words-based (BoVW)\\n', 'object\\n', 'classification~\\\\cite{jegou2010aggregating,delhumeau2013revisiting,arandjelovic2013all}\\n', 'and efficient box-encoding methods~\\\\cite{van2014fisher} to enable strong\\n', 'recognition performance.  The integration of this system with a monocular\\n', 'visual-SLAM (vSLAM) back-end also enables us to take advantage of both the\\n', 'reconstructed map and camera location to significantly bolster recognition\\n', 'performance. Additionally, our system design allows the run-time performance to\\n', 'be scalable to a larger number of object categories, with near-constant run-time\\n', 'for most practical object recognition tasks.\\n', '\\n', '% Lastly, we illustrate a few\\n', '% capabilities of our monocular visual-SLAM-aware object recognition system, such\\n', '% as providing \\\\textit{metrically-accurate maps} and \\\\textit{improved recognition\\n', '% efficiency}.\\n', '\\n', '% Taking a BoVW approach coupled\\n', '% with a selective-search strategy~\\\\cite{van2014fisher}, we avoid the need to\\n', '% perform traditional sliding-template-window\\n', '% techniques~\\\\cite{felzenszwalb2010object,dalal2005histograms}, thereby allowing\\n', '% the .\\n', '% Pipeline figure\\n', '%\\n', '\\\\begin{figure*}[!t]\\n', '    \\\\centering\\n', '    % \\\\def\\\\svgwidth{\\\\columnwidth}\\n', '    % \\\\input{graphics/slam_detect/pipeline.pdf_tex}\\n', '    \\\\includegraphics[width=2\\\\columnwidth]{graphics/slam_detect/pipeline.pdf}\\n', '    \\\\caption{Outline of the SLAM-aware object recognition pipeline. Given an\\n', 'input RGB image stream $\\\\mathcal{I}$, we first reconstruct the scene in a\\n', 'semi-dense fashion using an existing monocular visual-SLAM implementation\\n', '(ORB-SLAM) with a semi-dense depth estimator, and subsequently extract relevant\\n', 'map $\\\\mathcal{M}$, keyframe $\\\\mathcal{K}$ and pose information $\\\\xi$. We perform\\n', 'multi-scale density-based segmentation on the reconstructed scene to obtain\\n', 'object proposals $\\\\mathcal{O}$ that are consistent across multiple views. On\\n', 'each of the images in the input RGB image stream $\\\\mathcal{I}$, we compute\\n', 'Dense-SIFT ($\\\\mathbb{R}^{128}$) + RGB ($\\\\mathbb{R}^{3}$) and reduce it to $\\\\Phi\\n', '\\\\in \\\\mathbb{R}^{80}$ via PCA. The features $\\\\Phi$ are then used to efficiently\\n', 'encode each of the projected object proposals $\\\\mathcal{O}$ (bounding boxes of\\n', 'proposals projected on to each of the images with known poses $\\\\xi$) using VLAD\\n', 'with FLAIR, to obtain $\\\\Psi$. The resulting feature vector $\\\\Psi$ is used to\\n', 'train and predict likelihood of target label/category $p(x_i\\\\mid y)$ of the\\n', 'object contained in each of the object proposals. The likelihoods for each\\n', 'object $o \\\\in \\\\mathcal{O}$ are aggregated across each of the viewpoints $\\\\xi$ to\\n', 'obtain robust object category prediction.}\\n', '    \\\\label{fig:recognition-pipeline}\\n', '\\\\end{figure*}\\n', '%\\n', '%\\n', '\\n', 'We present several experimental results validating the improved object\\n', 'proposition and recognition performance of our proposed system: (i) The\\n', 'system is compared against the current\\n', 'state-of-the-art~\\\\cite{lai2012detection,laiunsupervised} on the UW-RGBD\\n', 'Scene~\\\\cite{lai2011large,laiunsupervised} Dataset. We compare the improved\\n', 'recognition performance of being SLAM-aware, to being SLAM-oblivious (ii) The\\n', 'multi-view object proposal method introduced is shown to outperform single-view\\n', 'object proposal strategies such as BING~\\\\cite{cheng2014bing} on the UW-RGBD\\n', 'dataset, that provide object candidates solely on a single-view. (iii) The\\n', 'run-time performance of our system is analysed, with specific discussion on the\\n', 'scalability of our approach, compared to existing state-of-the-art\\n', 'methods~\\\\cite{lai2012detection, laiunsupervised}.\\n', '\\n', '% (v) We show via experiments that our system is able to roughly learn the\\n', '% metric scales/sizes of objects using RGB-D training data, and use this\\n', '% information to feed as a weak prior to the scale-ambiguous monocular SLAM\\n', '% reconstruction, to finally provide a reasonably metrically-accurate map.\\n', '\\n', '%\\n', '% By taking\\n', '% advantage of the reconstructed map, the system can selectively choose to pick\\n', '% which image viewpoints to analyze, thereby making it more computationally efficient while\\n', '% retaining sufficient recognition performance. \\n', '% %\\n', '\\n', '% Secondly, by driving down the\\n', '% number of candidate regions that the detector is required to classify, and by\\n', '% introducing some of the recent advancements in \\n', '\\n', '% , where objects occurring in the\\n', '% environment \\n', '\\n', '\\n', '\\n', '\\n', '\\\\section{Related Work}\\n', '\\\\label{sec:related-work}\\n', '\\n', '% Object recognition has gained tremendous popularity recently with the resurgence\\n', '% of deep-learning methods\\\\footnote{We acknowledge the recent developments in this\\n', '% field, and intend to investigate these techniques in the future.}, and new\\n', '% feature encoding methods making them significantly richer in representing object\\n', '% categories and their instances. While a lot of these developments can certainly\\n', '% support perception-related tasks for robots, the computer vision community has\\n', '% predominantly focused its efforts on improving overall recognition performance\\n', '% on large-scale datasets that sometimes contain objects that do not concern with\\n', '% robots operating in our environments.\\n', '\\n', 'We discuss some of the recent developments in object \\n', 'proposals, recognition, \\n', 'and semi-dense monocular visual SLAM literature that has sparked the ideas explained in this paper.\\n', '\\n', '%\\n', '% ========================================= \\n', '%\\n', '\\\\textbf{Sliding window techniques and DPM} In traditional state-of-the-art\\n', 'object detection, HOG~\\\\cite{dalal2005histograms} and\\n', 'deformable-part-based-models (DPM) proposed by~\\\\citet{felzenszwalb2010object}\\n', 'have become the norm due to their success in recognition performance. These\\n', 'methods explicitly model the shape of each object and its parts via\\n', 'oriented-edge templates, across several scales. Despite its reduced\\n', 'dimensionality, the template model is scanned over the\\n', 'entire image in a sliding-window fashion across multiple scales for each object\\n', 'type that needs to be identified. This is a highly limiting\\n', 'factor in scalability, as the run-time performance of the system is directly\\n', 'dependent on the number of categories identifiable. While techniques have been\\n', 'proposed to scale such schemes to larger object categories~\\\\cite{dean2013fast},\\n', 'they incur a drop in recognition performance to trade-off for speed.\\n', '\\n', '% traditional classification schemes such as\\n', '% max-margin classifiers and stochastic-gradient-descent-based linear methods\\n', '\\n', '\\\\textbf{Dense sampling and feature encoding methods} Recently, many of the\\n', 'state-of-the-art techniques~\\\\cite{lazebnik2006beyond,van2014fisher} for generic\\n', 'object classification have resorted to dense feature extraction. Features are\\n', 'densely sampled on an image grid~\\\\cite{bosch2007image}, described, encoded and\\n', 'aggregated over the image or a region to provide a rich description of the\\n', 'object contained in it. The aggregated feature encodings lie as feature vectors\\n', 'in high-dimensional space, on which linear or kernel-based classification\\n', 'methods perform remarkably well. Among the most popular encoding schemes include\\n', 'Bag-of-Visual-Words (BoVW)~\\\\cite{csurka2004visual,sivic2003video}, and more\\n', 'recently Super-Vectors~\\\\cite{zhou2010image}, VLAD~\\\\cite{jegou2010aggregating},\\n', 'and Fisher Vectors~\\\\cite{perronnin2010improving}. In the case of BoVW, a\\n', 'histogram of occurrences of codes are built using a vocabulary of finite size $V\\n', '\\\\in \\\\mathbb{R}^{K \\\\times D}$. VLAD and Fisher Vectors, in contrast, aggregate\\n', 'residuals using the vocabulary to estimate the first and second order moment\\n', 'statistics in an attempt to reduce the loss of information introduced in the\\n', 'vector-quantization (VQ) step in BoVW. Both VLAD and Fisher Vectors have been\\n', 'shown to outperform traditional BoVW\\n', 'approaches~\\\\cite{jegou2010aggregating,perronnin2010improving,chatfield2011devil},\\n', 'and are used as a drop-in replacement to BoVW; we do the same utilizing VLAD as\\n', 'it provides a good trade-off between descriptiveness and computation time.\\n', '\\n', '% Bag-of-words-driven architectures have been predominantly\\n', '% limited due to their lack of object localization capabilities, and therefore\\n', '% naturally used in applications that require whole-image matching for\\n', '% mapping~\\\\cite{GalvezTRO12,cummins2011appearance}. \\n', '\\n', '\\\\textbf{Object Proposals} Recently, many of the\\n', 'state-of-the-art techniques in large-scale object recognition systems have\\n', 'argued the need for a category-independent object proposal method that provides candidate\\n', 'regions in images that may likely contain objects. Variants of these include\\n', 'Constrained-Parametric Min-cuts (CPMC)~\\\\cite{carreira2010constrained}, Selective\\n', 'Search~\\\\cite{uijlings2013selective}, Edge Boxes~\\\\cite{zitnick2014edge},\\n', 'Binarized Normed Gradients (BING)~\\\\cite{cheng2014bing}. The object candidates\\n', 'proposed are category-independent, and achieve detection rates (DR) of 95-99\\\\% at\\n', '0.7 intersection-over-union~(IoU\\\\footnote{Intersection-over-Union (IoU) is a\\n', 'common technique to evaluate the quality of candidate object proposals with\\n', 'respect to ground truth. The intersection area of the ground truth bounding box\\n', 'and that of the candidate is divided by the union of their areas.}) threshold, by generating about 1000-5000\\n', 'candidate proposal windows~\\\\cite{hosang2014good,zitnick2014edge}. This\\n', 'dramatically reduces the search space for existing sliding-window approaches\\n', 'that scan templates over the entire image, and across multiple scales; however, it\\n', 'still bodes a challenge to accurately classify irrelevant proposal windows as\\n', 'background.  For a thorough evaluation of the state-of-the-art object proposal\\n', 'methods, and their performance, we refer the reader to~\\\\citet{hosang2014good}.\\n', '\\n', '%  Recently, some of the best performing recognition systems have employed the use\\n', '% of object proposals to reduce the number of candidate windows to evaluate their\\n', '% detectors over, considerably improving detection\\n', '% performance~\\\\cite{alexe2012measuring,uijlings2013selective}.  These advancements\\n', '% have also suddenly \\n', '\\n', '\\\\textbf{Scalable Encoding with Object Proposals} As previously addressed,\\n', 'sliding-window techniques inherently deal with the scalability issue, despite\\n', 'recent schemes to speed-up such an approach. BoVW, on the contrary, handle this\\n', 'scalability issue rather nicely since the histograms do not particularly encode\\n', 'spatial relations as strongly. This however, makes BoVW approaches lack the\\n', 'ability to localize objects in an image. The advent of category-independent\\n', 'object proposal methods have subsequently opened the door to bag-of-words-driven\\n', 'architectures, where object proposal windows can now be described via existing\\n', 'feature encoding methods. Most recently,~\\\\citet{van2014fisher} employ a novel\\n', 'box-encoding technique using integral histograms to describe object proposal\\n', 'windows with a run-time independent of the window size of object proposals\\n', 'supplied. They report results with an 18x speedup over brute-force BoVW encoding\\n', '(for 30,000 object proposals), enabling a new state-of-the-art on the\\n', 'challenging 2010 PASCAL VOC detection task. Additionally their\\n', 'proposed system ranks number one in the official ImageNet 2013 detection\\n', 'challenge, making it a promising solution to consider for robotics applications.\\n', '\\n', '% While this dramatically reduces\\n', '% the search space for traditional sliding-window-based detectors, it can still\\n', '% pose a challenge to accurately classify each of the candidate proposals with high\\n', '% precision and recall.\\n', '\\n', '% While most existing algorithms employ\\n', '% superpixel-based on edge-based methods to inform the locations of objects in an\\n', '% image, there seems to a simple extension to the multi-view (continuous image\\n', '% stream) case that is lacking in the literature. Furthermore, \\n', '\\n', '% \\\\todosp{START off mentioning robots and\\n', '%   continuous observations in a scene, multi-view is advantageous. Multiple\\n', '%   detections can be aggregated using SLAM for improved performance, and improved\\n', '%   performance in detection can help\\n', '%   SLAM. Semantic\\n', '%   SFM}\\n', '\\n', '\\\\textbf{Multi-view Object Detection} While classical object detection methods\\n', 'focus on single-view-based recognition performance, some of these methods have\\n', 'been extended to the multi-view\\n', 'case~\\\\cite{thomas2006towards,collet2010efficient}, by aggregating object\\n', 'evidence across disparate views. \\\\citet{lai2012detection} proposed a\\n', 'multi-view-based approach for detecting and labeling objects in a 3D environment\\n', 'reconstructed using an RGB-D sensor. They utilize the popular HOG-based\\n', 'sliding-window detectors trained from object views in the RGB-D\\n', 'dataset~\\\\cite{lai2011large,laiunsupervised} to assign class probabilities to\\n', 'pixels in each of the frames of the RGB-D stream. Given co-registered image and\\n', 'depth, these probabilities are assigned to voxels in a discretized reconstructed\\n', '3D scene, and further smoothed using a Markov Random Field (MRF). Bao et\\n', 'al.~\\\\cite{bao2011semantic,bao2012semantic} proposed one of the first approaches\\n', 'to jointly estimate camera parameters, scene points and object labels using both\\n', 'geometric and semantic attributes in the scene. In their work, the authors\\n', 'demonstrate the improved object recognition performance, and robustness by\\n', 'estimating the object semantics and SfM jointly. However, the run-time of ~20\\n', 'minutes per image-pair, and the limited object categories identifiable makes the\\n', 'approach impractical for on-line robot operation. Other works\\n', '~\\\\cite{salas2013slam++,castle2010combining,civera2011towards,bo2011hierarchical,guptaECCV14}\\n', 'have also investigated object-based SLAM, SLAM-aware, and 3D object recognition\\n', 'architectures, however they have a few of glaring concerns: either (i) the\\n', 'system cannot scale beyond a finite set of object instances (generally limited\\n', 'to less than 10), or (ii) they require RGB-D input to support both detection and\\n', 'pose estimation, or (iii) they require rich object information such as 3D models\\n', 'in its database to match against object instances in a brute-force manner.\\n', '\\n', '% \\\\todosp{May be insert how our systems resolves these concerns here.}\\n', '% However, we investigate the ability to perform \\\\textit{scalable, visual SLAM-aware\\n', '% object recognition with the use of a single RGB-camera}, and show superior\\n', '% overall detection and recognition performance over vanilla or SLAM-oblivious\\n', '% object detection architectures that function on a per-frame recognition basis.\\n', '\\n', '% More importantly, we realize the\\n', '% need to move away from template-based detectors usually incorporated, primarily\\n', '% due to the scalability concern these detectors are known\\n', '% for~\\\\cite{sadeghi201430hz}. Instead we incorporate a scalable, and richer\\n', '% semantic representation~\\\\cite{van2014fisher} that is described on a pixel-level\\n', '% allowing for trivial-extensions to 3D for future work.\\n', '\\n', '% (Scalability issue)\\n', '% 3D scene understanding is a fundamental problem in\\n', '% robotics and perception, as knowledge of the environment\\n', '% is a prerequisite for complex robotic tasks and interactions.\\n', '% Thus far, most robotic scene understanding work has focused\\n', '% on outdoor scenes captured with laser scanners, where tech-\\n', '% nologies have matured and enabled high-impact applications\\n', '% such as mapping and autonomous driving [2], [25], [21], [7],\\n', '% [30].\\n', '% Advances in RGB-D matching and scene reconstruction\\n', '% make it feasible to go beyond single-view labeling and allow\\n', '% the continuous capture of a scene, where RGB-D videos\\n', '% can be robustly and efficiently merged into consistent and\\n', '% detailed 3D point clouds [18].\\n', '\\n', '% In this work, we emphasize an object-centric view of 3D\\n', '% labeling, combining techniques in object detection, scene\\n', '% labeling, and 3D reconstruction. Instead of learning from\\n', '% entire scenes, we train object detectors on view-based data,\\n', '% i.e. views of objects possibly isolated and/or from a different\\n', '% domain.\\n', '% Our technique combines both appearance and shape\\n', '% information without requiring annotated scenes or complete\\n', '% 3D object models for training.\\n', '\\n', '% Training a\\n', '% generic objectness measure to produce a small set of candidate object windows,\\n', '% has been shown to speed up the classical sliding window object detection\\n', '% paradigm. We observe that generic objects with well- defined closed boundary can\\n', '% be discriminated by looking at the norm of gradients, with a suitable resizing\\n', '% of their cor- responding image windows in to a small fixed size. Based on this\\n', '% observation and computational reasons, we propose to resize the window to 8 \\xc3\\x97 8\\n', '% and use the norm of the gra- dients as a simple 64D feature to describe it, for\\n', '% explicitly training a generic objectness measure.\\n', '\\n', '% Experiments on the challenging PASCAL VOC 2007 dataset\\n', '% show that our method efficiently (300fps on a single lap-\\n', '% top CPU) generates a small set of category-independent,\\n', '% high quality object windows, yielding 96.2% object detec-\\n', '% tion rate (DR) with 1,000 proposals. Increasing the num-\\n', '% bers of proposals and color spaces for computing BING fea-\\n', '% tures, our performance can be further improved to 99.5%\\n', '% DR.\\n', '\\n', '% Objectness is usually represented as a value which re-\\n', '% flects how likely an image window covers an object of any\\n', '% category [3]. A generic objectness measure has great po-\\n', '% tential to be used in a pre-filtering process to significantly\\n', '% improve: i) computational efficiency by reducing the search\\n', '% space, and ii) detection accuracy by allowing the usage of strong classifiers\\n', '% during testing. However, designing a\\n', '% good generic objectness measure method is difficult, which\\n', '% should:\\n', '\\n', '% \\xe2\\x80\\xa2 achieve high object detection rate (DR), as any unde-\\n', '% tected objects at this stage cannot be recovered later;\\n', '% \\xe2\\x80\\xa2 produce a small number of proposals for reducing\\n', '% computational time of subsequent detectors;\\n', '% \\xe2\\x80\\xa2 obtain high computational efficiency so that the\\n', '% method can be easily involved in various applications,\\n', '% especially for realtime and large-scale applications;\\n', '% \\xe2\\x80\\xa2 have good generalization ability to unseen object cat-\\n', '% egories, so that the proposals can be reused by many\\n', '% category specific detectors to greatly reduce the com-\\n', '% putation for each of them.\\n', '\\n', '% Van de Sande et al. [25] further improves selective\\n', '% search by adding VLAD [14]. Recently, Cinbis et al.\\n', '% [5] achieves state-of-the-art detection on PASCAL VOC\\n', '% 2010 using selective search in combination with reweighted\\n', '% Fisher vectors [18]. In [5], the main computational bottle-\\n', '% neck for detecting objects is the expensive encoding step\\n', '% for each box in the image. Moreover, in the references,\\n', '% Fisher and VLAD are shown to benefit from ` 2 or power-\\n', '% normalization, which implies that the feature vector of a box\\n', '% can no longer be merged from two smaller boxes. Hence\\n', '% in [25, 5] a brute-force approach is applied, made more\\n', '% efficient by the application of product quantization [13].\\n', '\\n', '% % =========================================\\n', '% \\\\textbf{BoVW with Selective Search}\\n', '% \\\\cite{van2014fisher}\\n', '\\n', '% By representing\\n', '% the picture as sparse integral images, one per codeword,\\n', '% we achieve a Fast Local Area Independent Representation.\\n', '% FLAIR allows for very fast evaluation of any box encoding\\n', '% and still enables spatial pooling. In FLAIR we achieve exact\\n', '% VLADs difference coding, even with ` 2 and power-norms.\\n', '\\n', '% we achieve ex-\\n', '% act and approximate Fisher vectors with FLAIR. The results\\n', '% are a 18x speedup, which enables us to set a new state-of-\\n', '% the-art on the challenging 2010 PASCAL VOC objects and\\n', '% the fine-grained categorization of the CUB-2011 200 bird\\n', '% species. Plus, we rank number one in the official ImageNet\\n', '% 2013 detection challenge.\\n', '\\n', '\\n', '% \\\\cite{xiao2013sun3d}\\n', '\\n', '\\n', '% % =========================================\\n', '% \\\\textbf{HOG and traditional object detection: Scalability concern}\\n', '% \\\\cite{felzenszwalb2010object},\\n', '% % \\\\cite{dean2013fast}, \\\\cite{uijlings2013selective}\\n', '          \\n', '% In object detection, HOG has proven to be successful\\n', '% in combination with the part-based model by Felzenszwalb\\n', '% et al. [10]. It models object shape templates and scans\\n', '% the image with boxes at multiple scales. Because more\\n', '% than 100,000 boxes need to be inspected per object type\\n', '% and aspect ratio, the analysis must be restricted to the low-\\n', '% dimensional HOG features or to simple histograms. Re-\\n', '% cently, Dean et al. [8] report an impressive speed-up for\\n', '% object detectors based on HOG part-templates, but they\\n', '% still require exhaustive scanning. Simple histograms can\\n', '% be efficiently computed with multi-dimensional integral im-\\n', '% ages [19], but use prohibitive amounts of memory at higher\\n', '% dimensionalities. Sub-window search [15] and selective\\n', '% search [24] opened the door to the use of locality with\\n', '% BoW, which is computationally more expensive than HOG\\n', '% [8] but superior in the quality of the semantic interpretation\\n', '% [15, 27, 12, 24, 25, 5]. Dean et al. achieve a speedup factor\\n', '% 20,000 at the cost of a drop in accuracy, we do a speedup\\n', '% factor of 18 but enabling an accuracy increase for the state-\\n', '% of-the-art in object detection.\\n', '\\n', '% ========================================= \\n', '\\n', '% % =========================================\\n', '% \\\\cite{lai2011large}, \\n', '% \\\\cite{girshick14CVPR}, \\\\cite{sadeghi201430hz}, \\\\cite{guptaECCV14},\\n', '% , \\n', '\\n', '% \\\\cite{ranganathan2007semantic}, \\\\cite{kundu2014joint}\\n', '% State-of-the-art detectors HOG (template based), RCNN, RCNN-depth\\n', '  % - Currently limited in run-time performance\\n', '  % - HOG: scalability issue with the need to train separate templates for each\\n', '  % class category\\n', '  %   - another issue with the need to deal with deformable parts for each\\n', '  %   category\\n', '  %   - DPM v5: tricks to make sota detectors run at reasonable speeds\\n', '  %   - no inherent variability in performance: single run time speed\\n', '\\n', '%   - CNNs, RCNNs, RCNN-depth very promising, and is progressing at a very fast\\n', '%   pace. Future work\\n', '\\n', '%   - Bag-of-words limited to whole-image description\\n', '%     - densely sampled features in the scene (usually 4 pixels apart)\\n', '%     - cannot be extended to object-level description\\n', '%     - shown to be limited in quality of description; utilize higher order\\n', '%     statistics in images to make more descriptive histograms such as VLAD, and\\n', '%     Fisher vectors. \\n', '%     - explain vlad, and fisher vectors\\n', '\\n', '% Past work: \\n', '%   - Kevin Lai, L. Bo, D. Fox with semantic labeling of a reconstructed scene\\n', '%   using HOG based templates. \\n', '%     - Reconstruction done using RGB-D data as input and SLAM back-end\\n', '%     - Traditional object detection using HOG templates and fusing information\\n', '%     across multiple views in a single voxelized space that is reconstructed \\n', '%     - Restrictions/Issues: \\n', '%       - Offline scene reconstruction, defeats the purpose of object detections\\n', '%       that may be useful for a robot in an online setting\\n', '%       - RGB-D data limited, not a big concern\\n', '%       - HOG-based templates: not very scalable (one template per category),\\n', '%       their results show only upto 5 object categories\\n', '\\n', '%     - HMP (same authors): \\n', '%       - Explicitly find features in 3D that can be discriminative \\n', '%       - Cannot learn \\n', '\\n', '\\n', '\\n', '\\n', '\\n', '%\\n', '\\\\begin{figure*}[!th]\\n', '  \\\\centering\\n', '  \\\\includegraphics[width=2\\\\columnwidth]{graphics/objectness/object_detect_slam.png}\\n', '  \\\\caption{An illustration of the multi-view object proposal method and\\n', '    subsequent SLAM-aware object recognition. Given an input RGB image stream, a\\n', '    scale-ambiguous semi-dense map is reconstructed (a) via the \\n', '    ORB-SLAM-based~\\\\cite{mur2015orb} semi-dense mapping solution. The reconstruction\\n', '    retains edges that are consistent across multiple views, and is employed in\\n', '    proposing objects directly from the reconstructed space. The resulting\\n', '    reconstruction is (b) filtered and (c) partitioned into several segments using a\\n', '    multi-scale density-based clustering approach that teases apart objects (while\\n', '    filtering out low-density regions) via the semi-dense edge-map\\n', '    reconstruction. Each of the clustered regions are then (d) projected on to each\\n', '    of individual frames in the original RGB image stream, and a bounded candidate\\n', '    region is proposed for subsequent feature description, encoding and\\n', '    classification. (e) The probabilities for each of the proposals per-frame are\\n', '    aggregated across multiple views to infer the most likely object label.}\\n', '  \\\\label{fig:multi-view-objectness}\\n', '\\\\end{figure*}\\n', '% \\n', '\\n', '\\\\section{Monocular SLAM Supported \\\\\\\\Object Recognition}\\n', '\\\\label{sec:proc-procedure}\\n', 'This section introduces the algorithmic components of our method.\\n', 'We refer the reader to Figure~\\\\ref{fig:recognition-pipeline} that illustrates the\\n', 'steps involved, and provide a brief overview of our system. \\n', '\\n', '\\\\subsection{Multi-view Object Proposals}\\n', '\\\\label{sec:proc-objectness} \\n', '\\n', 'Most object proposal strategies use either superpixel-based or edge-based\\n', 'representations to identify candidate proposal windows in a single image that\\n', 'may likely contain objects. Contrary to classical per-frame object proposal\\n', 'methodologies, robots observe the same instances of objects in its environment\\n', 'several times and from disparate viewpoints. It is natural to think of object\\n', 'proposals from a spatio-temporal or reconstructed 3D context, and a key\\n', 'realization is the added robustness that the temporal component provides in\\n', 'rejecting spatially inconsistent edge observations or candidate proposal\\n', 'regions. Recently,~\\\\citet{engel2014lsd} proposed a scale-drift aware monocular\\n', 'visual SLAM solution called LSD-SLAM, where the scenes are reconstructed in a\\n', 'semi-dense fashion, by fusing spatio-temporally consistent scene edges. Despite\\n', 'being scale-ambivalent, the multi-view reconstructions can be especially\\n', 'advantageous in teasing apart objects in the near-field versus those in the\\n', 'far-field regions, and thus subsequently be useful in identifying candidate\\n', 'object windows for a particular view. We build on top of\\n', 'an existing monocular SLAM solution (ORB-SLAM~\\\\cite{mur2015orb}) and augment a\\n', 'semi-dense depth filtering component derived from~\\\\cite{forster2014svo}. The\\n', 'resulting reconstruction qualitatively is similar to that produced by\\n', 'LSD-SLAM~\\\\cite{engel2014lsd}, and is used for subsequent object proposal\\n', 'generation. We avoided the use of LSD-SLAM as it occasionally failed over\\n', 'tracking wide-baseline motions inherent in the benchmark dataset we used. \\n', '\\n', '% by\\n', '% observing them over multiple viewpoints. \\\\todosp{FIX Keeping a\\n', '% \\\\textit{static-world} assumption, this is analogous to SfM or vSLAM \\n', '% vision-based SLAM, where consistent landmarks are observed and incorporated over\\n', '% multiple time instances and viewpoints.}\\n', '\\n', '% While traditional vSLAM has focused on feature-based methods, recently however,\\n', '% LSD-SLAM proposed by \\\\citet{engel2014lsd} has shown promising semi-dense\\n', '% large-scale scene reconstructions using a single monocular camera. These\\n', '% semi-dense representations explicitly reconstruct scale-ambiguous scene edges\\n', '% with a scale-drift aware formulation. \\\\todosp{The edges reconstructed in the\\n', '% scene are spatio-temporally consistent, and are }. \\n', '\\n', '\\n', 'In order to retrieve object candidates that are spatio-temporally consistent, we\\n', 'first perform a density-based partitioning on the scale-ambiguous reconstruction\\n', 'using both spatial and edge color information. This is done repeatedly for 4\\n', 'different density threshold values (each varied by a factor of 2), producing an\\n', 'over-segmentation of points in the reconstructed scene that are used as seeds\\n', 'for multi-view object candidate proposal. The spatial density segmentations\\n', 'eliminate any spurious points or edges in the scene, and the resulting point\\n', 'cloud is sufficient for object proposals. These object over-segmentation seeds\\n', 'are subsequently projected onto each of the camera views, and serve as seeds to\\n', 'for further occlusion handling, refinement and candidate object proposal\\n', 'generation. We cull out (i) small candidates whose window size is less than\\n', '20x20 px, (ii) occluding candidates by estimating their median depth from the\\n', 'reconstruction, to avoid mis-identification and (iii) overlapping candidates\\n', 'with an IoU threshold of 0.5, to avoid redundant proposals. The filtered set of\\n', 'windows are subsequently considered as candidates for the classification process\\n', 'downstream. Figure~\\\\ref{fig:multi-view-objectness} illustrates the different\\n', 'steps described in this section.\\n', '\\n', '\\n', '% With this in mind, we perform spatial clustering on the scale-ambiguous map\\n', '% using a na\\\\\"{\\\\i}ve density threshold that appropriately segments out candidate\\n', '% object proposals in the reconstructed map, and simultaneously rejects outliers in\\n', '% the form of spurious points or edges that are inconsistent across multiple\\n', '% viewpoints. These object candidates are subsequently projected onto each of the\\n', '% camera views and further culled on a frame-by-frame basis based on their window\\n', '% size. We cull out proposal regions that occupy less than 20px-wide\\n', '% windows in the image, and the resulting windows are considered as candidates for\\n', '% the classification process downstream. }\\n', '\\n', '\\n', '% Objectness generally refers to a measure of how\\n', '% likely a region in an image may contain an object of any category. Most\\n', '% state-of-the-art recognition systems require category-specific detectors that\\n', '% activate over many windows or bounding boxes in a sliding window\\n', '% fashion~\\\\cite{felzenszwalb2010object, dalal2005histograms}. \\n', '\\n', '\\n', '\\n', '\\\\subsection{State-of-the-art Bag-of-Visual-Words with Object Proposals}\\n', '\\\\label{subsec:proc-vlad-flair}\\n', '\\n', '\\n', 'Given the object proposals computed using the reconstructed scale-ambiguous map,\\n', 'we now direct our attention to describing these proposal regions. \\n', '\\n', '% Most recently,\\n', '% these object proposal strategies have opened the door to the use of\\n', '% bag-of-visual-words-based (BoVW) approaches that are known to be semantically\\n', '% richer in their representation. We take a similar approach: \\n', '\\n', '%\\n', '% VLAD FLAIR figure\\n', '%\\n', '\\\\begin{figure*}[!t]\\n', '    \\\\centering\\n', '    \\\\includegraphics[width=2\\\\columnwidth]{graphics/bow/vlad-flair.png}\\n', '    \\\\caption{Various steps involved in the feature extraction\\n', 'procedure. Features that are densely sampled from the image are subsequently\\n', 'used to describe the multi-view object proposals using FLAIR. Each proposal is\\n', 'described with multiple ([1x1], [2x2], [4x4]) spatial levels/bins via quick\\n', 'table lookups in the integral VLAD histograms (through FLAIR). The resulting\\n', 'histogram $\\\\Psi$ (after concatenation) is used to describe\\n', 'the object contained in the bounding box. Figure is best viewed in electronic form.}\\n', '    \\\\label{fig:feature-extraction}\\n', '\\\\end{figure*}\\n', '\\n', '\\\\textbf{Dense BoVW with VLAD} Given an input image\\n', 'and candidate object proposals, we first densely sample the image, describing\\n', 'each of the samples with SIFT + RGB color values, $\\\\Phi_{SIFT+RGB} \\\\in\\n', '\\\\mathbb{R}^{131}$ i.e. Dense SIFT~(128-D) + RGB(3-D). Features are extracted with a\\n', 'step size of 4 pixels, and at 4 different pyramid scales with a pyramid scale\\n', 'factor of $\\\\sqrt[]{2}$. The resulting description is then reduced to a\\n', '80-dimensional vector via PCA, called PCA-SIFT $\\\\Phi \\\\in \\\\mathbb{R}^{80}$. A\\n', 'vocabulary $V \\\\in \\\\mathbb{R}^{K\\\\times80}$ of size $K=64$ is created via\\n', '$k$-means, using the descriptions extracted from a shuffled subset of the training data,\\n', 'as done in classical bag-of-visual-words approaches. In classical BoVW, this\\n', 'vocabulary can be used to encode each of the original SIFT+RGB descriptions in\\n', 'an image into a histogram of occurrences of codewords, which in turn provides a\\n', 'compact description of the original image. Recently, however, more\\n', 'descriptive encodings such as VLAD~\\\\cite{jegou2010aggregating} and Fisher\\n', 'Vectors~\\\\cite{perronnin2010improving} have been shown to outperform classical\\n', 'BoVW\\n', 'approaches~\\\\cite{jegou2010aggregating,perronnin2010improving,chatfield2011devil}. Consequently,\\n', 'we chose to describe the features using VLAD as it provides equally as strong performance\\n', 'with slightly reduced computation time as compared to Fisher Vectors. \\n', '\\n', 'For each of the bounding boxes, the un-normalized VLAD $\\\\Psi \\\\in\\n', '\\\\mathbb{R}^{KD}$ description is computed\\n', 'by aggregating the residuals of each of the descriptions $\\\\Phi$ (enclosed within the\\n', 'bounding box) from their vector-quantized centers in the vocabulary, thereby\\n', 'determining its first order moment (Eq.~\\\\ref{eqn:vlad-first-order}). \\\\vspace{-1mm}\\n', '\\\\begin{align}\\n', 'v_k = \\\\sum_{x_i:NN(x_i)=\\\\mu_k} x_i - \\\\mu_k\\n', '\\\\label{eqn:vlad-first-order}\\n', '\\\\end{align} \\n', 'The\\n', 'description is then normalized using signed-square-rooting (SSR) or commonly\\n', 'known as power normalization (Eq.~\\\\ref{eq:signed-square-rooting}) with\\n', '$\\\\alpha=0.5$, followed by L2 normalization, for improved recognition performance\\n', 'as noted in~\\\\cite{arandjelovic2013all}.\\n', '\\\\begin{align} f(z) = sign(z)\\\\vert z \\\\vert^\\\\alpha \\\\quad \\\\text{where} \\\\quad 0 \\\\leq\\n', '\\\\alpha \\\\leq 1\\n', '\\\\label{eq:signed-square-rooting}\\n', '\\\\end{align} Additional descriptions for each bounding region are constructed for\\n', '3 different spatial bin levels or subdivisions as noted in\\n', '~\\\\cite{lazebnik2006beyond} (1x1, 2x2 and 4x4, 21 total subdivisions $S$), and\\n', 'stacked together to obtain the feature vector $\\\\Psi = \\\\begin{bmatrix}\\n', '\\\\hdots \\\\textbf{v}_s \\\\hdots \\\\end{bmatrix} \\\\in \\n', '\\\\mathbb{R}^{KDS}$ that appropriately describes the\\n', 'specific object contained within the candidate object proposal/bounding box.\\\\vspace{2mm}\\n', '\\n', '\\\\textbf{Efficient Feature Encoding with FLAIR} While it may be practical to\\n', 'describe a few object proposals in the scene with these encoding methods, it can\\n', 'be highly impractical to do so as the number of object proposals grows. To this\\n', 'end, \\\\citet{van2014fisher} introduced FLAIR - an encoding mechanism that\\n', 'utilizes summed-area tables of histograms to enable fast descriptions for\\n', 'arbitrarily many boxes in the image. By constructing integral histograms for\\n', 'each code in the codebook, the histograms or descriptions for an arbitrary\\n', 'number of boxes $B$ can be computed independent of their area. As shown\\n', 'in~\\\\cite{van2014fisher}, these descriptions can also be extended to the VLAD\\n', 'encoding technique. Additionally, FLAIR affords performing spatial pyramid\\n', 'binning rather naturally, with only requiring a few additional table look-ups,\\n', 'while being independent of the area of $B$. We refer the reader to\\n', 'Figure~\\\\ref{fig:feature-extraction} for an illustration of the steps involved in\\n', 'describing these candidate object proposals.\\\\vspace{2mm}\\n', '\\n', '\\\\textbf{Multi-class histogram classification} Given training examples, $(x_1, y_1),\\\\ldots,(x_n, y_n)$ where $x_i\\\\in\\\\mathbb{R}^{KDS}$ are the VLAD descriptions \\n', 'and $y_i\\\\in\\\\{1,\\\\ldots, \\\\mathcal{C}\\\\}$ are the ground truth target labels,\\n', 'we train a linear classifier using Stochastic Gradient Descent (SGD), given by:\\n', '\\\\begin{align} \\n', 'E(w) = \\\\frac{1}{n}\\\\sum_{i=1}^{n} L(y_i, f(x_i)) + \\\\alpha R(w)\\n', '\\\\label{eq:sgd-classifier}\\n', '\\\\end{align} \\\\vspace{-1mm} where $L(y_i, f(x_i)) =\\n', '\\\\text{log}\\\\Big(1+\\\\text{exp}(-y_i\\\\textbf{w}^T\\\\textbf{x}_i)\\\\Big)$ is the logistic\\n', 'loss function, $R(w) = \\\\frac{1}{2} \\\\sum_{i=1}^{n} \\\\textbf{w}^T\\\\textbf{w}$ is the L2-regularization term that penalizes model complexity, and $\\\\alpha\\n', '> 0$ is a non-negative hyperparameter that adjusts the L2 regularization. A\\n', 'one-versus-all strategy is taken to extend the classifiers to multi-class\\n', 'categorization. For hard-negative mining, we follow~\\\\cite{van2014fisher}\\n', 'closely, bootstrapping additional examples from wrongly classified negatives for\\n', '2 hard-negative mining epochs.\\n', '\\n', '\\n', '\\n', '\\n', '% Frame-based recognition\\n', '%\\n', '\\\\begin{figure*}[!tp]\\n', '    \\\\centering\\n', '    \\\\includegraphics[width=2\\\\columnwidth]{graphics/frame_detect/frame_detect.png}\\n', '    \\\\caption{Illustration of \\\\textbf{per-frame} detection results provided by our object\\n', '      recognition system that is \\\\textit{intentionally SLAM-oblivious} (for\\n', '      comparison purposes only). Object recognition evidence is\\n', '      not aggregated across all frames, and detections are performed on a\\n', '      frame-by-frame basis. Only detections having corresponding ground truth\\n', '      labels are shown. Figure is best viewed in electronic form. }\\n', '    \\\\label{fig:frame-detection-results}\\n', '\\\\end{figure*}\\n', '%\\n', '\\n', '% SLAM-based recognition\\n', '%\\n', '\\\\begin{figure*}[t]\\n', '    \\\\centering\\n', '    \\\\includegraphics[width=2\\\\columnwidth]{graphics/slam_detect/slam_detect-2.png}\\n', '    \\\\caption{Illustration of the recognition capabilities of our proposed\\n', '      SLAM-aware object recognition system. Each of the object categories are detected\\n', '      every frame, and their evidence is aggregated across the entire sequence through\\n', '      the set of object hypothesis. In frame-based object recognition, predictions are\\n', '      made on an individual image basis (shown in gray).  In SLAM-aware recognition,\\n', '      the predictions are aggregated across all frames in the image sequence to\\n', '      provide robust recognition performance. The green boxes indicate correctly\\n', '      classified object labels, and the gray boxes indicate background object\\n', '      labels. Figure is best viewed in electronic form.}\\n', '    \\\\label{fig:slam-detection-results-1}\\n', '\\\\end{figure*}\\n', '\\n', '\\n', '\\\\subsection{Multi-view Object Recognition}\\n', '\\\\label{sec:proc-multiview-recognition} We start with the ORB-SLAM-based\\n', 'semi-dense mapping solution, that feeds a continuous image stream, in order to\\n', 'recover a scale-ambiguous map~$\\\\mathcal{M}$, keyframes~$\\\\mathcal{K}$, and poses\\n', '${\\\\xi}$ corresponding to each of the frames in the input image stream. The\\n', 'resulting scale-ambiguous reconstruction provides a strong indicator of object\\n', 'presence in the environment, that we use to over-segment into several object\\n', 'seeds $o \\\\in \\\\{1, \\\\dots, \\\\mathcal{O}\\\\}$.  These object seeds are projected back\\n', 'in to each of the individual frames using the known projection matrix, derived\\n', 'from its corresponding viewpoint $\\\\xi_i$. The median depth estimates of each of\\n', 'the seeds are estimated in order to appropriately project non-occluding object\\n', 'proposals back in to corresponding viewpoint, using a depth buffer. Using these\\n', 'as candidate object proposals, we evaluate our detector on each of the\\n', '$\\\\mathcal{O}$ object clusters, per image, providing probability estimates of\\n', 'belonging to one of the $\\\\mathcal{C}$ object classes or categories. Thus, the\\n', 'maximum-likelihood estimate of the object $o \\\\in \\\\mathcal{O}$ can be formalized\\n', 'as maximizing the data-likelihood term for all observable viewpoints (assuming\\n', 'uniform prior across the $\\\\mathcal{C}$ classes):\\n', '\\\\begin{align}\\n', '\\\\hat{y}^{MLE} = \\\\underset{y\\\\in\\\\{1,\\\\dots,\\\\vert\\\\mathcal{C}\\\\vert\\\\}}{\\\\mathrm{argmax}}~p(\\\\mathcal{D}_{o}\\\\mid y) \\\\quad \\\\forall~o \\\\in \\\\mathcal{O}\\n', '\\\\label{eq:multi-view-evidence}\\n', '\\\\end{align}\\n', 'where $y\\\\in\\\\{1,\\\\dots,\\\\vert\\\\mathcal{C}\\\\vert\\\\}$ are\\n', 'the class labels, $\\\\mathcal{D}_{o} = \\\\{x_{1}, \\\\dots, x_{N}\\\\}_{o}$ is the data observed of the\\n', 'object cluster $o \\\\in \\\\mathcal{O}$ across $N$ observable viewpoints. In our\\n', 'case, $\\\\mathcal{D}_{o}$ refers to the bounding box of\\n', 'the $o^{th}$ cluster, projected onto each of the $N$\\n', 'observable viewpoints. Assuming the individual features in $\\\\mathcal{D}_{o}$ are conditionally\\n', 'independent given the class label $y$, the maximum-likelihood estimate (MLE)\\n', 'factorizes to: \\\\vspace{-2mm}\\n', '\\\\begin{align} \\n', '\\\\hat{y}^{MLE} &= \\\\underset{y\\\\in\\\\{1,\\\\dots,\\\\vert\\\\mathcal{C}\\\\vert\\\\}}{\\\\mathrm{argmax}} \\\\prod_{n=1}^N p(x_n\\\\mid y)\\\\\\\\ \\n', '                   &= \\\\underset{y\\\\in\\\\{1,\\\\dots,\\\\vert\\\\mathcal{C}\\\\vert\\\\}}{\\\\mathrm{argmax}} \\\\sum_{n=1}^N \\\\log p(x_n\\\\mid y)\\n', '\\\\label{eq:multi-view-evidence-2}\\n', '\\\\end{align} \\n', 'Thus the MLE of an object cluster $o$ belonging to one of the\\n', '$\\\\mathcal{C}$ classes, is the class that corresponds to having the highest of\\n', 'the sum of the log-likelihoods of their individual\\n', 'class probabilities estimated for each of the $N$ observable viewpoints.\\n', '\\n', '\\n', '% \\\\input{tex/sec-proc-implementation}\\n', '\\n', '% =========================================\\n', '\\n', '\\n', '% \\\\subsection{Post-Processing}\\n', '% \\\\begin{align}\\n', '% v_k &= \\\\frac{1}{M\\\\sqrt{\\\\pi_k}} \\\\sum_{i=1}^M \\\\gamma_k(x_i)~\\\\frac{x_i -\\n', '%   \\\\mu_k}{\\\\sigma_k}\\\\\\\\\\n', '% u_k &= \\\\frac{1}{M\\\\sqrt{2\\\\pi_k}} \\\\sum_{i=1}^M \\\\gamma_k(x_i)~\\\\bigg(\\\\frac{x_i -\\n', '%   \\\\mu_k}{\\\\sigma_k} - 1\\\\bigg)^2\\n', '% \\\\label{eqn:fisher-second-order}\\n', '% \\\\end{align}\\n', '% where $\\\\gamma_k(x_i)$.\\n', '\\n', '% \\\\textbf{Efficient Additive Kernels}: \\n', '\\n', '% \\\\textbf{Linear kernel}:\\n', '% \\\\begin{align}\\n', '% K(\\\\textbf{x},\\\\textbf{y}) = \\\\sum_{l=1}^D \\\\textbf{x}_l \\\\textbf{y}_l\\n', '% \\\\label{eqn:linear-kernel}\\n', '% \\\\end{align}\\n', '\\n', '\\n', '% \\\\section{SLAM-Aware Object Recognition}\\n', '% \\\\label{sec:proc-spatially-aware-object-recognition}\\n', '\\n', '% \\\\subsection{SLAM-Backed Robust Recognition}\\n', '% \\\\label{subsec:proc-slam-backed-robust-recognition}\\n', '\\n', '% \\\\subsection{Improved Efficiency via Active Recognition}\\n', '% \\\\label{subsec:proc-improved-efficiency-via-active-recognition}\\n', '\\n', '% \\\\subsection{Recognition-Aware SLAM and SLAM-Aware Recognition}\\n', '% \\\\label{subsec:proc-recog-slam}\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\\\section{Experiments}\\n', '\\\\label{sec:experiments}\\n', 'In this section, we evaluate the proposed SLAM-aware object\\n', 'recognition method. In our experiments, we extensively evaluate our SLAM-aware\\n', 'recognition system on the popular UW RGB-D\\n', 'Dataset (v2)\\\\cite{lai2011large,laiunsupervised}. We compare against the current\\n', 'state-of-the-art solution proposed by~\\\\citet{lai2012detection}, that utilize\\n', 'full map and camera location information for improved\\n', 'recognition performance. The UW RGB-D dataset contains a total 51 object\\n', 'categories, however, in order to maintain a fair comparison, we consider the\\n', 'same set of 5 objects as noted in~\\\\cite{lai2012detection}. In experiment 3, we\\n', 'propose scalable recognition solutions, increasing the number of objects\\n', 'considered to all 51 object categories in the UW RGB-D Dataset.\\\\vspace{1mm}\\n', '\\n', '% \\\\input{tex/sec-results-baseline}\\n', '\\n', '\\\\textbf{Experiment 1: SLAM-Aware Object Recognition Performance Evaluation}\\n', '\\\\label{experiment-1} We train and evaluate our system on the UW RGB-D Scene\\n', 'Dataset~\\\\cite{lai2011large,laiunsupervised}, providing mean-Average Precision\\n', '(mAP) estimates (see Table~\\\\ref{tab:mAP-SLAM}) for the object recognition task\\n', 'and compare against existing methods~\\\\cite{lai2012detection}. We split our\\n', 'experiments into two categories: \\\\vspace{1mm} \\n', '\\n', '\\\\input{results/mAP-slam}\\n', '\\n', '\\\\textit{(i) Single-View recognition performance: } First, we evaluate the\\n', 'recognition performance of our proposed system on each of the scenes in the\\n', 'UW-RGB-D Scene Dataset on a per-frame basis, detecting and classifying objects\\n', 'that occur every 5 frames in each scene (as done\\n', 'in~\\\\cite{lai2012detection}). Each object category is trained from images in the\\n', 'Object Dataset, that includes several viewpoints of object instances with their\\n', 'corresponding mask, and category information. Using training parameters\\n', 'identical to the previous experiment, we achieve a performance of\\n', '81.5 mAP as compared to the detector performance of 61.7 mAP\\n', 'reported in~\\\\cite{lai2012detection}. Recognition is done on a per-image\\n', 'basis, and averaged across all test images for reporting.\\n', 'Figure~\\\\ref{fig:frame-detection-results} shows the recognition results of our\\n', 'system on a per-frame basis. We ignore regions labeled as background in the\\n', 'figure for clarity and only report the correct and incorrect predictions in\\n', 'green and red respectively. \\\\vspace{1mm}\\n', '\\n', '\\\\textit{(ii) Multi-View recognition performance: } In this section, we\\n', 'investigate the performance of a SLAM-aware object recognition system. We\\n', 'compare this to a SLAM-oblivious object detector described previously, and\\n', 'evaluate using ground truth provided. Using the poses $\\\\xi$ and reconstructed\\n', 'map $\\\\mathcal{M}$, multi-view object candidates are proposed and projected onto\\n', 'each of the images for each scene sequence. Using the candidates provided as\\n', 'input to the recognition system, the system predicts the likelihood and\\n', 'corresponding category of an object (including background) contained in a\\n', 'candidate bounding box. For each of the objects $o \\\\in \\\\mathcal{O}$ proposed,\\n', 'the summed log-likelihood is computed (as in Eqn.~\\\\ref{eq:multi-view-evidence})\\n', 'to estimate the most likely object category over all the images for a particular\\n', 'scene sequence. We achieve 89.8 mAP recognition performance on the 5 objects in\\n', 'each of the scenes in~\\\\cite{laiunsupervised} that was successfully reconstructed\\n', 'by the ORB-SLAM-based semi-dense mapping\\n', 'system. Figures~\\\\ref{fig:intro-fig},~\\\\ref{fig:multi-view-objectness},\\n', '~\\\\ref{fig:slam-detection-results-1} and~\\\\ref{fig:more-slam-detect} illustrate\\n', 'the capabilities of the proposed system in providing robust recognition\\n', 'performance by taking advantage of the monocular visual\\n', 'SLAM-backend. Figure~\\\\ref{fig:mAP-SLAM} illustrates the average precision-recall\\n', 'performance on the UW RGB-D dataset, comparing the classical frame-based and our\\n', 'SLAM-aware approach. As expected, with additional object viewpoints, our\\n', 'proposed SLAM-aware solution predicts with improved precision and recall. In\\n', 'comparison to that of HMP2D+3D~\\\\cite{laiunsupervised}, they achieve only\\n', 'slightly higher overall recognition performance of 90.9 mAP, as their\\n', 'recognition pipeline takes advantage of the RGB and depth input to improve\\n', 'overall scene reconstruction. We do note that while we perform comparably with\\n', 'HMP2D+3D~\\\\cite{laiunsupervised}, our BoVW+FLAIR architecture allows our system\\n', 'to scale to a large number of object categories with \\\\textit{near-constant\\n', 'run-time}. We investigate the run-time performance and scalability concerns\\n', 'further in Experiment 3.  \\\\vspace{1mm}\\n', '\\n', '\\n', '%\\n', '% \\\\begin{figure}[!h]\\n', '%     \\\\centering\\n', '%     % {\\\\setlength{\\\\tabcolsep}{1mm}\\n', '%     \\\\begin{tabular}{cc}\\n', '%       \\\\hspace{-2mm}\\n', '%       \\\\includegraphics[width=0.5\\\\columnwidth]{results/frame_slam_average_pr.pdf}\\n', '%       \\\\hspace{-2mm}\\n', '%       \\\\includegraphics[width=0.5\\\\columnwidth]{results/multiview_pr.pdf}\\n', '%     \\\\end{tabular}\\n', '%     \\\\caption{Precision-Recall curves for frame-based detection compared to that\\n', '%       of our proposed SLAM-aware approach. \\\\textbf{Left:} PR curves indicating recognition\\n', '%       performance for each object category using our multi-view object proposals, and\\n', '%       VLAD+FLAIR feature encoding on the UW-RGBD dataset. \\\\textbf{Right:}\\n', '%       Performance comparison via precision-recall for the frame-based vs. SLAM-aware object\\n', '%       recognition. As expected, the performance of our proposed SLAM-aware\\n', '%       solution increases with more recognition evidence accumulated across\\n', '%       multiple viewpoints. }\\n', '%     \\\\label{fig:slam-detection-pr}\\n', '%   % }\\n', '% \\\\end{figure}\\n', '\\n', '\\n', '%\\n', '%\\n', '\\n', '% %\\n', '% \\\\begin{figure*}[!t]\\n', '%     \\\\centering\\n', '%     \\\\includegraphics[width=2\\\\columnwidth]{graphics/slam_detect/slam_detect.png}\\n', '%     \\\\caption{More illustrations of strong performance results achieved via our\\n', '%       SLAM-aware recognition system. Figure is best viewed in electronic form.}\\n', '%     \\\\label{fig:slam-detection-results-2}\\n', '% \\\\end{figure*}\\n', '% %\\n', '% %\\n', '\\n', '\\n', '\\\\begin{figure}[b]\\n', '  \\\\centering\\n', '  \\\\vspace{2mm}\\n', '    \\\\includegraphics[width=0.95\\\\columnwidth]{graphics/objectness/objectness.pdf} \\n', '    \\\\caption{\\\\textit{Varying number of proposals:} We experiment with varied\\n', '      number of bounding boxes for the BING object proposal method, and compare\\n', '      against our multi-view object proposal method that uses considerably fewer\\n', '      number of bounding boxes to get similar or better recall rates. The numbers next\\n', '      to the label indicate the average number of windows proposed in the image. }\\n', '    \\\\label{fig:objectness-performance}\\n', '\\\\end{figure}\\n', '\\n', '\\\\textbf{Experiment 2: Multi-View Objectness}\\n', '\\\\label{subsec:slam-driven-objectness} In this experiment, we investigate the\\n', 'effectiveness of our multi-view object proposal method in identifying\\n', 'category-independent objects in a continuous video stream. We compare the recall\\n', 'of our object proposal method with the recently introduced\\n', 'BING~\\\\cite{cheng2014bing} object proposal technique, whose performance in\\n', 'detection rate (DR) and run-time claim to be promising. We compare against the\\n', 'BING method, varying the number of proposed object candidates by picking\\n', 'proposals in descending order of their objectness\\n', 'score. Figure~\\\\ref{fig:objectness-performance} compares the overall performance\\n', 'of our multi-view object proposal method that achieves better recall\\n', 'rates, for a particular IoU threshold with\\n', 'considerably fewer object proposals. The results provided are evaluated\\n', 'on all the scenes provided in the UW-RGB-D dataset\\n', '(v2)~\\\\cite{laiunsupervised}. \\n', '% \\\\vspace{2mm}\\n', '\\n', '% Some of results are illustrated in Figure~\\\\ref{fig:multi-view-objectness-2}.\\\\vspace{1mm}\\n', '\\n', '\\n', '\\\\textbf{Experiment 3: Scalable recognition and run-time evaluation}\\n', '\\\\label{subsec:results-run-time-perf} In this section, we investigate the\\n', 'run-time performance of computing VLAD with integral histograms (FLAIR) for our\\n', 'system and compare against previously proposed\\n', 'approaches~\\\\cite{van2014fisher,lai2012detection}. We measure the average speed\\n', 'for feature-extraction (Dense-SIFT) and feature-encoding (VLAD) as they take up\\n', 'over 95\\\\% of the overall compute time. All experiments were conducted with a\\n', 'single-thread on an Intel Core-i7-3920XM (2.9GHz). \\n', '\\n', '% The runtimes indicated in the\\n', '% FLAIR feature encoding step includes the vector-quantization step to find the closest\\n', '% code words.\\n', '\\n', '% We experiment with\\n', '% varied dense sampling step sizes of 2, 4, 6, 8 and 10 px, and several several\\n', '% descriptor types including SIFT~\\\\cite{lowe2004distinctive},\\n', \"% OpponentSIFT\\\\footnote{The ``Opponent'' prefix indicates that the descriptors\\n\", '% were extracted across 3 channels in the Opponent color space.},\\n', '% ORB~\\\\cite{rublee2011orb}, and OpponentORB, and report the run-times for\\n', '% each. \\n', '% See\\n', '% figure~\\\\ref{fig:runtime-performance} for runtime evaluation experiment\\n', '% description.\\n', '\\n', '% We\\n', '% maintain a fixed vocabulary size of $K=64$, and extract features densely over 4\\n', '% pyramid scales, with a scale factor of $\\\\sqrt[]{2}$. We also fix the spatial\\n', '% pyramids constant, subdividing each window/bounding-box to 1x1, 2x2 and 4x4\\n', '% grids. \\n', '\\n', '\\\\input{results/runtime.tex}\\n', '\\n', '\\\\citet{van2014fisher} reports that the overall feature extraction\\n', 'and encoding takes 5.15s (VQ 0.55s, FLAIR construction 0.6s, VLAD+FLAIR 4.0s)\\n', 'per image, with the following parameters (2px step size, 3 Pyr. Scales,\\n', '[1x1],~[4x4] spatial pyramid bins). With significantly fewer candidate\\n', 'proposals, and careful implementation, our system is able to achieve the same\\n', '(with 4px step size) in\\n', 'approximately 1.6s. With reference to~\\\\cite{lai2012detection}, where the\\n', 'run-time performance of the sliding-window approach is directly proportional to\\n', 'the number of object categories detectable, the authors report an overall\\n', 'run-time of 1.8s for 5 object categories. However, scaling up their detection to\\n', 'larger number of objects would imply costly runtimes, making it highly\\n', 'impractical for real-time purposes. The run-time of our approach (based\\n', 'on~\\\\cite{van2014fisher}), on the other hand, is scalable to a larger number of\\n', 'object categories, making it a strong contender for real-time recognition\\n', 'systems. We summarize the run-times of our approach compared to that\\n', 'of~\\\\cite{lai2012detection} and~\\\\cite{laiunsupervised} in Table~\\\\ref{table:runtime}. \\\\vspace{1mm}\\n', '\\n', '% Additionally, as seen\\n', '% in Figure~\\\\ref{fig:runtime-performance}, we also\\n', '% note that it may be especially advantageous for robots to \\\\textit{be able to vary\\n', '% parameters such as step size, and descriptor type that directly affect the\\n', '% run-time performance of the system, at the cost of accuracy, depending on the\\n', '% task at hand}. \\n', '\\n', '% \\\\begin{figure}[h]\\n', '%   \\\\centering\\n', '%     \\\\includegraphics[width=\\\\columnwidth]{graphics/runtime/runtime.pdf} \\n', '%     \\\\caption{\\\\textit{Varying step size and descriptors:} Plots illustrating the\\n', '%       run-time performance of our system with varied step size and descriptors}\\n', '%     \\\\label{fig:runtime-performance}\\n', '% \\\\end{figure}\\n', '\\n', '\\n', '% RGBD mapping and HOG based feature extraction\\n', '% Running time. The RGB-D Mapping algorithm [18] used\\n', '% for scene reconstruction runs in real-time for our Kinect\\n', '% videos, which were collected at 15-20 Hz. We evaluate object\\n', '% detectors on every 10th frame, or every 0.5 seconds. Our\\n', '% current single-threaded MATLAB implementation is not yet\\n', '% real-time, requiring 4 seconds to process each frame. Vox-\\n', '% elization and graph cut inference take negligible time. The\\n', '% overwhelming majority of computation is spent on feature\\n', '% extraction and sliding window classification, each taking\\n', '% around 1.8 seconds.\\n', '\\n', '\\n', '\\n', '% % Cloud Objectness figure\\n', '% %\\n', '% \\\\begin{figure*}[!t]\\n', '%     \\\\centering\\n', '%     \\\\includegraphics[width=2\\\\columnwidth]{graphics/objectness/labeled/scene-labels.png}\\n', '%     \\\\caption{More illustrations of the multi-view object proposals method. Each\\n', '%       scene is reconstructed in a semi-dense fashion using our ORB-SLAM-based\\n', '%       semi-dense mapping solution,\\n', '%       and over-segmented using a multi-scaled spatial density segmentation. 4th and 5th\\n', '%       columns on the first 2 rows: The resulting\\n', '%       over-segmentations are used as seeds (in red) to propose candidate windows in the\\n', '%       image (to cover object parts that failed to reconstruct). Randomized\\n', '%       Prim~\\\\cite{manen2013prime} is used to propose 3-4 candidate windows\\n', '%       corresponding to the seed. Last row: Few examples of top 3 candidate\\n', '%       windows proposed. The last two columns show failures in the proposal generation}\\n', '%     \\\\label{fig:multi-view-objectness-2}\\n', '% \\\\end{figure*}\\n', '% %\\n', '\\n', '\\n', '\\n', '\\n', '% \\\\subsection{Scalable Recognition}\\n', '% \\\\label{subsec:results-scalable-recog}\\n', '% \\\\textbf{Experiment 3: Recognition performance with varied number of object categories }\\n', '\\n', '% \\\\subsection{Efficient-Active Recognition}\\n', '% \\\\label{subsec:results-active-recog}\\n', '% \\\\textbf{Experiment 4: Improved recognition efficiency}\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\\\begin{figure}[h]\\n', '  \\\\centering\\n', '    \\\\includegraphics[width=\\\\columnwidth]{graphics/slam_detect/slam_detect-4.png} \\n', '    \\\\caption{More illustrations of the superior performance of the\\n', '        SLAM-aware object recognition in scenarios of ambiguity and\\n', '        occlusions. The coffee mug is misidentified as a soda can, and the cap\\n', '        in the bottom row is occluded by the cereal box.}\\n', '    \\\\label{fig:more-slam-detect}\\n', '\\\\end{figure}\\n', '\\n', '\\\\textbf{Discussion and Future Work} While there are benefits to running a\\n', 'monocular visual SLAM-backend for recognition purposes, the inter-dependence of\\n', 'the recognition system on this backend makes it vulnerable to the same\\n', 'robustness concerns that pertain to monocular visual SLAM. In our experiments,\\n', 'we noticed inadequacies in the semi-dense vSLAM implementation that failed to\\n', 'reconstruct the scene on few occasions. To further emphasize recognition\\n', 'scalability, we are actively collecting a larger scaled dataset (in increased\\n', 'map area, and number of objects) to show the extent of capabilities of the\\n', 'proposed system. Furthermore, we realize the importance of real-time\\n', 'capabilities of such recognition systems, and intend to generalize the\\n', 'architecture to a streaming approach in the near future. We also hope to release\\n', 'the source code for our proposed method, allowing scalable and customizable\\n', 'training with fast run-time performance during live operation.\\n', '\\n', '% We continue to\\n', '% perform evaluations on datasets that present large variability in object\\n', '% categories and viewpoints. \\n', '\\n', '% We do note however, that the LSD-SLAM\\n', '% system used was not robust to the aggressive motions in this specific\\n', '% dataset. \\n', '\\n', '\\\\section{Conclusion} In this work, we develop a SLAM-aware object-recognition\\n', 'system, that is able to provide robust and scalable recognition performance as\\n', 'compared to classical SLAM-oblivious recognition methods. We leverage some of\\n', 'the recent advancements in semi-dense monocular SLAM to propose objects in the\\n', 'environment, and incorporate efficient feature encoding techniques to provide an\\n', 'improved object recognition solution whose run-time is \\\\textit{nearly-constant}\\n', 'to the number of objects identifiable by the system. Through various\\n', 'evaluations, we show that our SLAM-aware monocular recognition solution is\\n', 'competitive to current state-of-the-art in the {RGB-D} object recognition\\n', 'literature. We believe that robots equipped with such a monocular system will be\\n', 'able to robustly recognize and accordingly act on objects in their environment,\\n', 'in spite of object clutter and recognition ambiguity inherent from certain\\n', 'object viewpoint angles.\\n', '~\\n', '\\n', '\\n', '\\n', '% UNCOMMENT FOR CAMERA-READY SUBMISSION\\n', '\\\\section*{Acknowledgments} This work was funded by the Office of Naval Research\\n', 'under grants MURI N00014-10-1-0936, N00014-11-1-0688 and N00014-13-1-0588 and by\\n', 'the National Science Foundation under Award IIS-1318392. We would like to thank\\n', 'the authors of ORB-SLAM and LSD-SLAM for providing source code of their work,\\n', 'and the authors of the UW-RGB-D Dataset~\\\\cite{lai2012detection,laiunsupervised}\\n', 'for their considerable efforts in collecting, annotating and developing\\n', 'benchmarks for the dataset. \\\\pagebreak\\n', '\\n', '\\n', '% REFERENCES\\n', '% -----------------------------------------------------------------------\\n', '%% Use plainnat to work nicely with natbib. \\n', '\\n', '\\\\bibliographystyle{abbrvnat_ordered} % plainnat\\n', '{\\\\small\\n', '\\\\bibliography{tex/references}\\n', '}\\n', '\\n', '% END\\n', '% -----------------------------------------------------------------------\\n', '\\\\end{document}\\n', '\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "print content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = [x.replace('\\n', ' ') for x in content] \n",
    "# content = [x.append('') for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\\\RequirePackage{snapshot} ', '\\\\documentclass[conference]{IEEEtran} ', ' ', '% Packages ', '% ----------------------------------------------------------------------- ', ' ', '% \\\\newenvironment{ppl}{\\\\fontfamily{phv}\\\\selectfont}{\\\\par} ', '\\\\usepackage{amsfonts,amsmath,amssymb,mathtools} ', '\\\\usepackage{flushend} ', '\\\\usepackage{bigints} ', '\\\\usepackage{hhline} ', '\\\\usepackage{marvosym} % Mail symbol ', ' ', ' ', '% Font ', '% ----------------------------------------------------------------------- ', '% \\\\usepackage{times} ', '\\\\usepackage{tgpagella} ', '\\\\usepackage[T1]{fontenc} ', ' ', ' ', '% Graphics ', '% ----------------------------------------------------------------------- ', '\\\\usepackage{graphicx} ', '\\\\usepackage{booktabs, multicol, multirow} ', '\\\\usepackage{indentfirst} ', '\\\\usepackage{subfigure} ', '\\\\usepackage[font=footnotesize]{caption} ', '\\\\DeclareCaptionLabelFormat{andtable}{#1~#2  \\\\&  \\\\tablename~\\\\thetable} ', '\\\\DeclareCaptionLabelFormat{andfigure}{\\\\tablename~\\\\thetable~\\\\&~#1~#2} ', ' ', '% Inlined commenting command ', '% ----------------------------------------------------------------------- ', '\\\\usepackage{marginnote} ', '\\\\usepackage[usenames,dvipsnames]{color} ', '\\\\usepackage[table]{xcolor}    % loads also \\xc2\\xbbcolortbl\\xc2\\xab ', '\\\\setlength{\\\\marginparwidth}{0.5in} ', '\\\\definecolor{fullred}{rgb}{0.85,.0,.1} \\\\newcommand{\\\\fr}{\\\\color{fullred}} ', '\\\\definecolor{navyblue}{rgb}{.0,.0,.5} ', '\\\\definecolor{bleudefrance}{rgb}{0.19, 0.55, 0.91} ', '\\\\definecolor{bluegray}{rgb}{0.18, 0.36, 0.6} ', '% \\\\newcommand{\\\\todosp}[1]{\\\\marginpar{\\\\tiny\\\\noindent{\\\\raggedright\\\\color{blue} ', '%             {[SP]}\\\\fr{ #1}} \\\\par}} ', '\\\\newcommand{\\\\todoinsp}[1]{{\\\\noindent{\\\\raggedright\\\\color{blue} ', '      {[SP]}\\\\fr{ #1}} \\\\par}} ', ' ', '\\\\newcommand{\\\\todosp}[1]{{\\\\color{bluegray}{#1}}} ', ' ', '\\\\usepackage[textsize=tiny]{todonotes} ', ' ', '% Bold row for tables ', '% ----------------------------------------------------------------------- ', '\\\\usepackage{array} ', '\\\\newcolumntype{+}{>{\\\\global\\\\let\\\\currentrowstyle\\\\relax}} ', '\\\\newcolumntype{^}{>{\\\\currentrowstyle}} ', '\\\\newcommand{\\\\rowstyle}[1]{\\\\gdef\\\\currentrowstyle{#1}% ', '#1\\\\ignorespaces ', '} ', '\\\\newcolumntype{C}[1]{>{\\\\centering\\\\arraybackslash}p{#1}} ', ' ', '% Bib, Multicol, refs ', '% numbers option provides compact numerical references in the text.  ', '% ----------------------------------------------------------------------- ', '\\\\usepackage[numbers,sort]{natbib} ', '\\\\usepackage[bookmarks=true, colorlinks=true, urlcolor=Blue]{hyperref} ', '\\\\usepackage{multicol} ', ' ', ' ', '% Custom defined variables for general purpose needs ', '% ----------------------------------------------------------------------- ', '% UNCOMMENT FOR CAMERA-READY SUBMISSION ', '\\\\newcommand{\\\\ME}{Sudeep Pillai} ', '\\\\newcommand{\\\\PAPERAUTHORS}{Sudeep Pillai and John J. Leonard} ', '\\\\newcommand{\\\\PAPERTITLE}{Monocular SLAM Supported Object Recognition} ', '\\\\newcommand{\\\\PAPERTITLEFMT}{Monocular SLAM Supported\\\\\\\\Object Recognition} ', '\\\\newcommand{\\\\PAPERKEYWORDS}{Object Recognition; Computer Vision; Perception} ', ' ', '% http://tex.stackexchange.com/questions/48067/pdfinfo-doesnt-appear-to-be-working ', \"% hyperref and pdfinfo don't work together, use either \", '\\\\hypersetup{pdfauthor={\\\\ME},% ', '            pdftitle={\\\\PAPERTITLE},% ', '            pdfsubject={\\\\PAPERTITLE},% ', '            pdfkeywords={\\\\PAPERKEYWORDS},% ', '            pdfproducer={LaTeX},% ', '            pdfcreator={pdfLaTeX} ', '} ', ' ', '\\\\setlength{\\\\textfloatsep}{2pt} ', ' ', '% BEGIN ', '% ----------------------------------------------------------------------- ', '\\\\begin{document} ', ' ', '% paper title ', '\\\\title{\\\\PAPERTITLEFMT{}\\\\vspace{-0.5ex}} ', ' ', '% \\\\authorrefmark{1} ', ' ', '% UNCOMMENT FOR CAMERA-READY SUBMISSION ', '\\\\author{\\\\authorblockN{\\\\PAPERAUTHORS{}} ', '\\\\authorblockA{ ', 'Computer Science and Artificial Intelligence Laboratory\\\\\\\\ ', 'Massachusetts Institute of Technology\\\\\\\\ ', '\\\\{\\\\href{mailto:spillai@csail.mit.edu}{spillai}, \\\\href{mailto:jleonard@csail.mit.edu}{jleonard}\\\\}@csail.mit.edu}} ', ' ', '% ----------------------------------------------------------------------- ', '\\\\maketitle ', ' ', '% ABSTRACT ', '% ----------------------------------------------------------------------- ', '\\\\vspace{-1ex} ', '\\\\begin{abstract} ', 'In this work, we develop a monocular SLAM-aware object recognition system that ', 'is able to achieve considerably stronger recognition performance, as compared to ', 'classical object recognition systems that function on a frame-by-frame basis. By ', 'incorporating several key ideas including multi-view object proposals and ', 'efficient feature encoding methods, our proposed system is able to detect and ', 'robustly recognize objects in its environment using a single RGB camera in ', 'near-constant time. Through experiments, we illustrate the utility of using such ', 'a system to effectively detect and recognize objects, incorporating multiple ', 'object viewpoint detections into a unified prediction hypothesis. The ', 'performance of the proposed recognition system is evaluated on the UW RGB-D ', 'Dataset, showing strong recognition performance and scalable run-time ', 'performance compared to current state-of-the-art recognition systems. ', ' ', '\\\\end{abstract} ', ' ', '\\\\IEEEpeerreviewmaketitle ', ' ', '% CONTENT ', '% ----------------------------------------------------------------------- ', '\\\\section{Introduction} ', \"\\\\label{sec-introduction} Object recognition is a vital component in a robot's \", 'repertoire of skills. Traditional object recognition methods have focused on ', 'improving recognition performance (Precision-Recall, or mean Average-Precision) ', 'on specific datasets~\\\\cite{everingham2010pascal, ILSVRC15}. While these datasets ', 'provide sufficient variability in object categories and instances, the training ', 'data mostly consists of images of arbitrarily picked scenes and/or ', 'objects. Robots, on the other hand, perceive their environment as a continuous ', 'image stream, observing the same object several times, and from multiple ', 'viewpoints, as it constantly moves around in its immediate environment. As a ', 'result, object detection and recognition can be further bolstered if the ', 'robot were capable of simultaneously localizing itself and mapping (SLAM) its ', 'immediate environment - by integrating object detection evidences ', 'across multiple views. ', ' ', '% Additionally, robots can observe their immediate surroundings via these ', '% streams to localize and map itself quite robustly, presenting a far-more ', '% structured data representation compared to arbitrary hand-labeled images of ', '% object instances on the web.  ', ' ', ' ', '% A robot would benefit from the ability to identify objects in its immediate ', '% surroundings, including those it has observed in its recent past, to act upon ', '% them when necessary.  ', ' ', '% Robots, however, function in rich and contextual spaces, with the data observed ', '% being highly structured. The structured data refers to the concept of observed ', '% data being spatially and temporally correlated.   ', ' ', ' ', '% Intro figure ', '% ', '\\\\begin{figure}[!t] ', '  \\\\centering ', '  \\\\begin{tabular}{c} ', '    \\\\includegraphics[width=\\\\columnwidth]{graphics/slam_detect/intro.pdf} ', '  \\\\end{tabular} ', '  \\\\caption{The proposed SLAM-aware object recognition system is able to robustly ', '    localize and recognize several objects in the scene, aggregating detection evidence across multiple views. Annotations in white are provided for clarity and are actual predictions ', '    proposed by our system.} ', '  \\\\label{fig:intro-fig} ', '\\\\end{figure} ', ' ', ' ', ' ', '% \\\\textit{Using an RGB camera as the only input, we propose a SLAM-aware ', '% object recognition system that is considerably stronger in performance than a ', '% classical recognition system that detects objects on a per-frame basis}.  ', ' ', \"We refer to a ``SLAM-aware'' system as - one that has access to the map of its \", 'observable surroundings as it builds it incrementally and the location of its ', 'camera at any point in time. This is in contrast to classical recognition ', \"systems that are ``SLAM-oblivious'' - those that detect and recognize objects on \", 'a frame-by-frame basis without being cognizant of the map of its environment, the ', 'location of its camera, or that objects may be situated within these ', 'maps. \\\\textit{In this paper, we develop the ability for a SLAM-aware system to ', 'robustly recognize objects in its environment, using an RGB camera as its only ', 'sensory input} (Figure~\\\\ref{fig:intro-fig}). ', ' ', 'We make the following contributions towards this end: Using state-of-the-art ', 'semi-dense map reconstruction techniques in monocular visual SLAM as ', 'pre-processed input, we introduce the capability to propose multi-view ', 'consistent object candidates, as the camera observes instances of objects across ', 'several disparate viewpoints. Leveraging this object proposal method, we ', 'incorporate some of the recent advancements in bag-of-visual-words-based (BoVW) ', 'object ', 'classification~\\\\cite{jegou2010aggregating,delhumeau2013revisiting,arandjelovic2013all} ', 'and efficient box-encoding methods~\\\\cite{van2014fisher} to enable strong ', 'recognition performance.  The integration of this system with a monocular ', 'visual-SLAM (vSLAM) back-end also enables us to take advantage of both the ', 'reconstructed map and camera location to significantly bolster recognition ', 'performance. Additionally, our system design allows the run-time performance to ', 'be scalable to a larger number of object categories, with near-constant run-time ', 'for most practical object recognition tasks. ', ' ', '% Lastly, we illustrate a few ', '% capabilities of our monocular visual-SLAM-aware object recognition system, such ', '% as providing \\\\textit{metrically-accurate maps} and \\\\textit{improved recognition ', '% efficiency}. ', ' ', '% Taking a BoVW approach coupled ', '% with a selective-search strategy~\\\\cite{van2014fisher}, we avoid the need to ', '% perform traditional sliding-template-window ', '% techniques~\\\\cite{felzenszwalb2010object,dalal2005histograms}, thereby allowing ', '% the . ', '% Pipeline figure ', '% ', '\\\\begin{figure*}[!t] ', '    \\\\centering ', '    % \\\\def\\\\svgwidth{\\\\columnwidth} ', '    % \\\\input{graphics/slam_detect/pipeline.pdf_tex} ', '    \\\\includegraphics[width=2\\\\columnwidth]{graphics/slam_detect/pipeline.pdf} ', '    \\\\caption{Outline of the SLAM-aware object recognition pipeline. Given an ', 'input RGB image stream $\\\\mathcal{I}$, we first reconstruct the scene in a ', 'semi-dense fashion using an existing monocular visual-SLAM implementation ', '(ORB-SLAM) with a semi-dense depth estimator, and subsequently extract relevant ', 'map $\\\\mathcal{M}$, keyframe $\\\\mathcal{K}$ and pose information $\\\\xi$. We perform ', 'multi-scale density-based segmentation on the reconstructed scene to obtain ', 'object proposals $\\\\mathcal{O}$ that are consistent across multiple views. On ', 'each of the images in the input RGB image stream $\\\\mathcal{I}$, we compute ', 'Dense-SIFT ($\\\\mathbb{R}^{128}$) + RGB ($\\\\mathbb{R}^{3}$) and reduce it to $\\\\Phi ', '\\\\in \\\\mathbb{R}^{80}$ via PCA. The features $\\\\Phi$ are then used to efficiently ', 'encode each of the projected object proposals $\\\\mathcal{O}$ (bounding boxes of ', 'proposals projected on to each of the images with known poses $\\\\xi$) using VLAD ', 'with FLAIR, to obtain $\\\\Psi$. The resulting feature vector $\\\\Psi$ is used to ', 'train and predict likelihood of target label/category $p(x_i\\\\mid y)$ of the ', 'object contained in each of the object proposals. The likelihoods for each ', 'object $o \\\\in \\\\mathcal{O}$ are aggregated across each of the viewpoints $\\\\xi$ to ', 'obtain robust object category prediction.} ', '    \\\\label{fig:recognition-pipeline} ', '\\\\end{figure*} ', '% ', '% ', ' ', 'We present several experimental results validating the improved object ', 'proposition and recognition performance of our proposed system: (i) The ', 'system is compared against the current ', 'state-of-the-art~\\\\cite{lai2012detection,laiunsupervised} on the UW-RGBD ', 'Scene~\\\\cite{lai2011large,laiunsupervised} Dataset. We compare the improved ', 'recognition performance of being SLAM-aware, to being SLAM-oblivious (ii) The ', 'multi-view object proposal method introduced is shown to outperform single-view ', 'object proposal strategies such as BING~\\\\cite{cheng2014bing} on the UW-RGBD ', 'dataset, that provide object candidates solely on a single-view. (iii) The ', 'run-time performance of our system is analysed, with specific discussion on the ', 'scalability of our approach, compared to existing state-of-the-art ', 'methods~\\\\cite{lai2012detection, laiunsupervised}. ', ' ', '% (v) We show via experiments that our system is able to roughly learn the ', '% metric scales/sizes of objects using RGB-D training data, and use this ', '% information to feed as a weak prior to the scale-ambiguous monocular SLAM ', '% reconstruction, to finally provide a reasonably metrically-accurate map. ', ' ', '% ', '% By taking ', '% advantage of the reconstructed map, the system can selectively choose to pick ', '% which image viewpoints to analyze, thereby making it more computationally efficient while ', '% retaining sufficient recognition performance.  ', '% % ', ' ', '% Secondly, by driving down the ', '% number of candidate regions that the detector is required to classify, and by ', '% introducing some of the recent advancements in  ', ' ', '% , where objects occurring in the ', '% environment  ', ' ', ' ', ' ', ' ', '\\\\section{Related Work} ', '\\\\label{sec:related-work} ', ' ', '% Object recognition has gained tremendous popularity recently with the resurgence ', '% of deep-learning methods\\\\footnote{We acknowledge the recent developments in this ', '% field, and intend to investigate these techniques in the future.}, and new ', '% feature encoding methods making them significantly richer in representing object ', '% categories and their instances. While a lot of these developments can certainly ', '% support perception-related tasks for robots, the computer vision community has ', '% predominantly focused its efforts on improving overall recognition performance ', '% on large-scale datasets that sometimes contain objects that do not concern with ', '% robots operating in our environments. ', ' ', 'We discuss some of the recent developments in object  ', 'proposals, recognition,  ', 'and semi-dense monocular visual SLAM literature that has sparked the ideas explained in this paper. ', ' ', '% ', '% =========================================  ', '% ', '\\\\textbf{Sliding window techniques and DPM} In traditional state-of-the-art ', 'object detection, HOG~\\\\cite{dalal2005histograms} and ', 'deformable-part-based-models (DPM) proposed by~\\\\citet{felzenszwalb2010object} ', 'have become the norm due to their success in recognition performance. These ', 'methods explicitly model the shape of each object and its parts via ', 'oriented-edge templates, across several scales. Despite its reduced ', 'dimensionality, the template model is scanned over the ', 'entire image in a sliding-window fashion across multiple scales for each object ', 'type that needs to be identified. This is a highly limiting ', 'factor in scalability, as the run-time performance of the system is directly ', 'dependent on the number of categories identifiable. While techniques have been ', 'proposed to scale such schemes to larger object categories~\\\\cite{dean2013fast}, ', 'they incur a drop in recognition performance to trade-off for speed. ', ' ', '% traditional classification schemes such as ', '% max-margin classifiers and stochastic-gradient-descent-based linear methods ', ' ', '\\\\textbf{Dense sampling and feature encoding methods} Recently, many of the ', 'state-of-the-art techniques~\\\\cite{lazebnik2006beyond,van2014fisher} for generic ', 'object classification have resorted to dense feature extraction. Features are ', 'densely sampled on an image grid~\\\\cite{bosch2007image}, described, encoded and ', 'aggregated over the image or a region to provide a rich description of the ', 'object contained in it. The aggregated feature encodings lie as feature vectors ', 'in high-dimensional space, on which linear or kernel-based classification ', 'methods perform remarkably well. Among the most popular encoding schemes include ', 'Bag-of-Visual-Words (BoVW)~\\\\cite{csurka2004visual,sivic2003video}, and more ', 'recently Super-Vectors~\\\\cite{zhou2010image}, VLAD~\\\\cite{jegou2010aggregating}, ', 'and Fisher Vectors~\\\\cite{perronnin2010improving}. In the case of BoVW, a ', 'histogram of occurrences of codes are built using a vocabulary of finite size $V ', '\\\\in \\\\mathbb{R}^{K \\\\times D}$. VLAD and Fisher Vectors, in contrast, aggregate ', 'residuals using the vocabulary to estimate the first and second order moment ', 'statistics in an attempt to reduce the loss of information introduced in the ', 'vector-quantization (VQ) step in BoVW. Both VLAD and Fisher Vectors have been ', 'shown to outperform traditional BoVW ', 'approaches~\\\\cite{jegou2010aggregating,perronnin2010improving,chatfield2011devil}, ', 'and are used as a drop-in replacement to BoVW; we do the same utilizing VLAD as ', 'it provides a good trade-off between descriptiveness and computation time. ', ' ', '% Bag-of-words-driven architectures have been predominantly ', '% limited due to their lack of object localization capabilities, and therefore ', '% naturally used in applications that require whole-image matching for ', '% mapping~\\\\cite{GalvezTRO12,cummins2011appearance}.  ', ' ', '\\\\textbf{Object Proposals} Recently, many of the ', 'state-of-the-art techniques in large-scale object recognition systems have ', 'argued the need for a category-independent object proposal method that provides candidate ', 'regions in images that may likely contain objects. Variants of these include ', 'Constrained-Parametric Min-cuts (CPMC)~\\\\cite{carreira2010constrained}, Selective ', 'Search~\\\\cite{uijlings2013selective}, Edge Boxes~\\\\cite{zitnick2014edge}, ', 'Binarized Normed Gradients (BING)~\\\\cite{cheng2014bing}. The object candidates ', 'proposed are category-independent, and achieve detection rates (DR) of 95-99\\\\% at ', '0.7 intersection-over-union~(IoU\\\\footnote{Intersection-over-Union (IoU) is a ', 'common technique to evaluate the quality of candidate object proposals with ', 'respect to ground truth. The intersection area of the ground truth bounding box ', 'and that of the candidate is divided by the union of their areas.}) threshold, by generating about 1000-5000 ', 'candidate proposal windows~\\\\cite{hosang2014good,zitnick2014edge}. This ', 'dramatically reduces the search space for existing sliding-window approaches ', 'that scan templates over the entire image, and across multiple scales; however, it ', 'still bodes a challenge to accurately classify irrelevant proposal windows as ', 'background.  For a thorough evaluation of the state-of-the-art object proposal ', 'methods, and their performance, we refer the reader to~\\\\citet{hosang2014good}. ', ' ', '%  Recently, some of the best performing recognition systems have employed the use ', '% of object proposals to reduce the number of candidate windows to evaluate their ', '% detectors over, considerably improving detection ', '% performance~\\\\cite{alexe2012measuring,uijlings2013selective}.  These advancements ', '% have also suddenly  ', ' ', '\\\\textbf{Scalable Encoding with Object Proposals} As previously addressed, ', 'sliding-window techniques inherently deal with the scalability issue, despite ', 'recent schemes to speed-up such an approach. BoVW, on the contrary, handle this ', 'scalability issue rather nicely since the histograms do not particularly encode ', 'spatial relations as strongly. This however, makes BoVW approaches lack the ', 'ability to localize objects in an image. The advent of category-independent ', 'object proposal methods have subsequently opened the door to bag-of-words-driven ', 'architectures, where object proposal windows can now be described via existing ', 'feature encoding methods. Most recently,~\\\\citet{van2014fisher} employ a novel ', 'box-encoding technique using integral histograms to describe object proposal ', 'windows with a run-time independent of the window size of object proposals ', 'supplied. They report results with an 18x speedup over brute-force BoVW encoding ', '(for 30,000 object proposals), enabling a new state-of-the-art on the ', 'challenging 2010 PASCAL VOC detection task. Additionally their ', 'proposed system ranks number one in the official ImageNet 2013 detection ', 'challenge, making it a promising solution to consider for robotics applications. ', ' ', '% While this dramatically reduces ', '% the search space for traditional sliding-window-based detectors, it can still ', '% pose a challenge to accurately classify each of the candidate proposals with high ', '% precision and recall. ', ' ', '% While most existing algorithms employ ', '% superpixel-based on edge-based methods to inform the locations of objects in an ', '% image, there seems to a simple extension to the multi-view (continuous image ', '% stream) case that is lacking in the literature. Furthermore,  ', ' ', '% \\\\todosp{START off mentioning robots and ', '%   continuous observations in a scene, multi-view is advantageous. Multiple ', '%   detections can be aggregated using SLAM for improved performance, and improved ', '%   performance in detection can help ', '%   SLAM. Semantic ', '%   SFM} ', ' ', '\\\\textbf{Multi-view Object Detection} While classical object detection methods ', 'focus on single-view-based recognition performance, some of these methods have ', 'been extended to the multi-view ', 'case~\\\\cite{thomas2006towards,collet2010efficient}, by aggregating object ', 'evidence across disparate views. \\\\citet{lai2012detection} proposed a ', 'multi-view-based approach for detecting and labeling objects in a 3D environment ', 'reconstructed using an RGB-D sensor. They utilize the popular HOG-based ', 'sliding-window detectors trained from object views in the RGB-D ', 'dataset~\\\\cite{lai2011large,laiunsupervised} to assign class probabilities to ', 'pixels in each of the frames of the RGB-D stream. Given co-registered image and ', 'depth, these probabilities are assigned to voxels in a discretized reconstructed ', '3D scene, and further smoothed using a Markov Random Field (MRF). Bao et ', 'al.~\\\\cite{bao2011semantic,bao2012semantic} proposed one of the first approaches ', 'to jointly estimate camera parameters, scene points and object labels using both ', 'geometric and semantic attributes in the scene. In their work, the authors ', 'demonstrate the improved object recognition performance, and robustness by ', 'estimating the object semantics and SfM jointly. However, the run-time of ~20 ', 'minutes per image-pair, and the limited object categories identifiable makes the ', 'approach impractical for on-line robot operation. Other works ', '~\\\\cite{salas2013slam++,castle2010combining,civera2011towards,bo2011hierarchical,guptaECCV14} ', 'have also investigated object-based SLAM, SLAM-aware, and 3D object recognition ', 'architectures, however they have a few of glaring concerns: either (i) the ', 'system cannot scale beyond a finite set of object instances (generally limited ', 'to less than 10), or (ii) they require RGB-D input to support both detection and ', 'pose estimation, or (iii) they require rich object information such as 3D models ', 'in its database to match against object instances in a brute-force manner. ', ' ', '% \\\\todosp{May be insert how our systems resolves these concerns here.} ', '% However, we investigate the ability to perform \\\\textit{scalable, visual SLAM-aware ', '% object recognition with the use of a single RGB-camera}, and show superior ', '% overall detection and recognition performance over vanilla or SLAM-oblivious ', '% object detection architectures that function on a per-frame recognition basis. ', ' ', '% More importantly, we realize the ', '% need to move away from template-based detectors usually incorporated, primarily ', '% due to the scalability concern these detectors are known ', '% for~\\\\cite{sadeghi201430hz}. Instead we incorporate a scalable, and richer ', '% semantic representation~\\\\cite{van2014fisher} that is described on a pixel-level ', '% allowing for trivial-extensions to 3D for future work. ', ' ', '% (Scalability issue) ', '% 3D scene understanding is a fundamental problem in ', '% robotics and perception, as knowledge of the environment ', '% is a prerequisite for complex robotic tasks and interactions. ', '% Thus far, most robotic scene understanding work has focused ', '% on outdoor scenes captured with laser scanners, where tech- ', '% nologies have matured and enabled high-impact applications ', '% such as mapping and autonomous driving [2], [25], [21], [7], ', '% [30]. ', '% Advances in RGB-D matching and scene reconstruction ', '% make it feasible to go beyond single-view labeling and allow ', '% the continuous capture of a scene, where RGB-D videos ', '% can be robustly and efficiently merged into consistent and ', '% detailed 3D point clouds [18]. ', ' ', '% In this work, we emphasize an object-centric view of 3D ', '% labeling, combining techniques in object detection, scene ', '% labeling, and 3D reconstruction. Instead of learning from ', '% entire scenes, we train object detectors on view-based data, ', '% i.e. views of objects possibly isolated and/or from a different ', '% domain. ', '% Our technique combines both appearance and shape ', '% information without requiring annotated scenes or complete ', '% 3D object models for training. ', ' ', '% Training a ', '% generic objectness measure to produce a small set of candidate object windows, ', '% has been shown to speed up the classical sliding window object detection ', '% paradigm. We observe that generic objects with well- defined closed boundary can ', '% be discriminated by looking at the norm of gradients, with a suitable resizing ', '% of their cor- responding image windows in to a small fixed size. Based on this ', '% observation and computational reasons, we propose to resize the window to 8 \\xc3\\x97 8 ', '% and use the norm of the gra- dients as a simple 64D feature to describe it, for ', '% explicitly training a generic objectness measure. ', ' ', '% Experiments on the challenging PASCAL VOC 2007 dataset ', '% show that our method efficiently (300fps on a single lap- ', '% top CPU) generates a small set of category-independent, ', '% high quality object windows, yielding 96.2% object detec- ', '% tion rate (DR) with 1,000 proposals. Increasing the num- ', '% bers of proposals and color spaces for computing BING fea- ', '% tures, our performance can be further improved to 99.5% ', '% DR. ', ' ', '% Objectness is usually represented as a value which re- ', '% flects how likely an image window covers an object of any ', '% category [3]. A generic objectness measure has great po- ', '% tential to be used in a pre-filtering process to significantly ', '% improve: i) computational efficiency by reducing the search ', '% space, and ii) detection accuracy by allowing the usage of strong classifiers ', '% during testing. However, designing a ', '% good generic objectness measure method is difficult, which ', '% should: ', ' ', '% \\xe2\\x80\\xa2 achieve high object detection rate (DR), as any unde- ', '% tected objects at this stage cannot be recovered later; ', '% \\xe2\\x80\\xa2 produce a small number of proposals for reducing ', '% computational time of subsequent detectors; ', '% \\xe2\\x80\\xa2 obtain high computational efficiency so that the ', '% method can be easily involved in various applications, ', '% especially for realtime and large-scale applications; ', '% \\xe2\\x80\\xa2 have good generalization ability to unseen object cat- ', '% egories, so that the proposals can be reused by many ', '% category specific detectors to greatly reduce the com- ', '% putation for each of them. ', ' ', '% Van de Sande et al. [25] further improves selective ', '% search by adding VLAD [14]. Recently, Cinbis et al. ', '% [5] achieves state-of-the-art detection on PASCAL VOC ', '% 2010 using selective search in combination with reweighted ', '% Fisher vectors [18]. In [5], the main computational bottle- ', '% neck for detecting objects is the expensive encoding step ', '% for each box in the image. Moreover, in the references, ', '% Fisher and VLAD are shown to benefit from ` 2 or power- ', '% normalization, which implies that the feature vector of a box ', '% can no longer be merged from two smaller boxes. Hence ', '% in [25, 5] a brute-force approach is applied, made more ', '% efficient by the application of product quantization [13]. ', ' ', '% % ========================================= ', '% \\\\textbf{BoVW with Selective Search} ', '% \\\\cite{van2014fisher} ', ' ', '% By representing ', '% the picture as sparse integral images, one per codeword, ', '% we achieve a Fast Local Area Independent Representation. ', '% FLAIR allows for very fast evaluation of any box encoding ', '% and still enables spatial pooling. In FLAIR we achieve exact ', '% VLADs difference coding, even with ` 2 and power-norms. ', ' ', '% we achieve ex- ', '% act and approximate Fisher vectors with FLAIR. The results ', '% are a 18x speedup, which enables us to set a new state-of- ', '% the-art on the challenging 2010 PASCAL VOC objects and ', '% the fine-grained categorization of the CUB-2011 200 bird ', '% species. Plus, we rank number one in the official ImageNet ', '% 2013 detection challenge. ', ' ', ' ', '% \\\\cite{xiao2013sun3d} ', ' ', ' ', '% % ========================================= ', '% \\\\textbf{HOG and traditional object detection: Scalability concern} ', '% \\\\cite{felzenszwalb2010object}, ', '% % \\\\cite{dean2013fast}, \\\\cite{uijlings2013selective} ', '           ', '% In object detection, HOG has proven to be successful ', '% in combination with the part-based model by Felzenszwalb ', '% et al. [10]. It models object shape templates and scans ', '% the image with boxes at multiple scales. Because more ', '% than 100,000 boxes need to be inspected per object type ', '% and aspect ratio, the analysis must be restricted to the low- ', '% dimensional HOG features or to simple histograms. Re- ', '% cently, Dean et al. [8] report an impressive speed-up for ', '% object detectors based on HOG part-templates, but they ', '% still require exhaustive scanning. Simple histograms can ', '% be efficiently computed with multi-dimensional integral im- ', '% ages [19], but use prohibitive amounts of memory at higher ', '% dimensionalities. Sub-window search [15] and selective ', '% search [24] opened the door to the use of locality with ', '% BoW, which is computationally more expensive than HOG ', '% [8] but superior in the quality of the semantic interpretation ', '% [15, 27, 12, 24, 25, 5]. Dean et al. achieve a speedup factor ', '% 20,000 at the cost of a drop in accuracy, we do a speedup ', '% factor of 18 but enabling an accuracy increase for the state- ', '% of-the-art in object detection. ', ' ', '% =========================================  ', ' ', '% % ========================================= ', '% \\\\cite{lai2011large},  ', '% \\\\cite{girshick14CVPR}, \\\\cite{sadeghi201430hz}, \\\\cite{guptaECCV14}, ', '% ,  ', ' ', '% \\\\cite{ranganathan2007semantic}, \\\\cite{kundu2014joint} ', '% State-of-the-art detectors HOG (template based), RCNN, RCNN-depth ', '  % - Currently limited in run-time performance ', '  % - HOG: scalability issue with the need to train separate templates for each ', '  % class category ', '  %   - another issue with the need to deal with deformable parts for each ', '  %   category ', '  %   - DPM v5: tricks to make sota detectors run at reasonable speeds ', '  %   - no inherent variability in performance: single run time speed ', ' ', '%   - CNNs, RCNNs, RCNN-depth very promising, and is progressing at a very fast ', '%   pace. Future work ', ' ', '%   - Bag-of-words limited to whole-image description ', '%     - densely sampled features in the scene (usually 4 pixels apart) ', '%     - cannot be extended to object-level description ', '%     - shown to be limited in quality of description; utilize higher order ', '%     statistics in images to make more descriptive histograms such as VLAD, and ', '%     Fisher vectors.  ', '%     - explain vlad, and fisher vectors ', ' ', '% Past work:  ', '%   - Kevin Lai, L. Bo, D. Fox with semantic labeling of a reconstructed scene ', '%   using HOG based templates.  ', '%     - Reconstruction done using RGB-D data as input and SLAM back-end ', '%     - Traditional object detection using HOG templates and fusing information ', '%     across multiple views in a single voxelized space that is reconstructed  ', '%     - Restrictions/Issues:  ', '%       - Offline scene reconstruction, defeats the purpose of object detections ', '%       that may be useful for a robot in an online setting ', '%       - RGB-D data limited, not a big concern ', '%       - HOG-based templates: not very scalable (one template per category), ', '%       their results show only upto 5 object categories ', ' ', '%     - HMP (same authors):  ', '%       - Explicitly find features in 3D that can be discriminative  ', '%       - Cannot learn  ', ' ', ' ', ' ', ' ', ' ', '% ', '\\\\begin{figure*}[!th] ', '  \\\\centering ', '  \\\\includegraphics[width=2\\\\columnwidth]{graphics/objectness/object_detect_slam.png} ', '  \\\\caption{An illustration of the multi-view object proposal method and ', '    subsequent SLAM-aware object recognition. Given an input RGB image stream, a ', '    scale-ambiguous semi-dense map is reconstructed (a) via the  ', '    ORB-SLAM-based~\\\\cite{mur2015orb} semi-dense mapping solution. The reconstruction ', '    retains edges that are consistent across multiple views, and is employed in ', '    proposing objects directly from the reconstructed space. The resulting ', '    reconstruction is (b) filtered and (c) partitioned into several segments using a ', '    multi-scale density-based clustering approach that teases apart objects (while ', '    filtering out low-density regions) via the semi-dense edge-map ', '    reconstruction. Each of the clustered regions are then (d) projected on to each ', '    of individual frames in the original RGB image stream, and a bounded candidate ', '    region is proposed for subsequent feature description, encoding and ', '    classification. (e) The probabilities for each of the proposals per-frame are ', '    aggregated across multiple views to infer the most likely object label.} ', '  \\\\label{fig:multi-view-objectness} ', '\\\\end{figure*} ', '%  ', ' ', '\\\\section{Monocular SLAM Supported \\\\\\\\Object Recognition} ', '\\\\label{sec:proc-procedure} ', 'This section introduces the algorithmic components of our method. ', 'We refer the reader to Figure~\\\\ref{fig:recognition-pipeline} that illustrates the ', 'steps involved, and provide a brief overview of our system.  ', ' ', '\\\\subsection{Multi-view Object Proposals} ', '\\\\label{sec:proc-objectness}  ', ' ', 'Most object proposal strategies use either superpixel-based or edge-based ', 'representations to identify candidate proposal windows in a single image that ', 'may likely contain objects. Contrary to classical per-frame object proposal ', 'methodologies, robots observe the same instances of objects in its environment ', 'several times and from disparate viewpoints. It is natural to think of object ', 'proposals from a spatio-temporal or reconstructed 3D context, and a key ', 'realization is the added robustness that the temporal component provides in ', 'rejecting spatially inconsistent edge observations or candidate proposal ', 'regions. Recently,~\\\\citet{engel2014lsd} proposed a scale-drift aware monocular ', 'visual SLAM solution called LSD-SLAM, where the scenes are reconstructed in a ', 'semi-dense fashion, by fusing spatio-temporally consistent scene edges. Despite ', 'being scale-ambivalent, the multi-view reconstructions can be especially ', 'advantageous in teasing apart objects in the near-field versus those in the ', 'far-field regions, and thus subsequently be useful in identifying candidate ', 'object windows for a particular view. We build on top of ', 'an existing monocular SLAM solution (ORB-SLAM~\\\\cite{mur2015orb}) and augment a ', 'semi-dense depth filtering component derived from~\\\\cite{forster2014svo}. The ', 'resulting reconstruction qualitatively is similar to that produced by ', 'LSD-SLAM~\\\\cite{engel2014lsd}, and is used for subsequent object proposal ', 'generation. We avoided the use of LSD-SLAM as it occasionally failed over ', 'tracking wide-baseline motions inherent in the benchmark dataset we used.  ', ' ', '% by ', '% observing them over multiple viewpoints. \\\\todosp{FIX Keeping a ', '% \\\\textit{static-world} assumption, this is analogous to SfM or vSLAM  ', '% vision-based SLAM, where consistent landmarks are observed and incorporated over ', '% multiple time instances and viewpoints.} ', ' ', '% While traditional vSLAM has focused on feature-based methods, recently however, ', '% LSD-SLAM proposed by \\\\citet{engel2014lsd} has shown promising semi-dense ', '% large-scale scene reconstructions using a single monocular camera. These ', '% semi-dense representations explicitly reconstruct scale-ambiguous scene edges ', '% with a scale-drift aware formulation. \\\\todosp{The edges reconstructed in the ', '% scene are spatio-temporally consistent, and are }.  ', ' ', ' ', 'In order to retrieve object candidates that are spatio-temporally consistent, we ', 'first perform a density-based partitioning on the scale-ambiguous reconstruction ', 'using both spatial and edge color information. This is done repeatedly for 4 ', 'different density threshold values (each varied by a factor of 2), producing an ', 'over-segmentation of points in the reconstructed scene that are used as seeds ', 'for multi-view object candidate proposal. The spatial density segmentations ', 'eliminate any spurious points or edges in the scene, and the resulting point ', 'cloud is sufficient for object proposals. These object over-segmentation seeds ', 'are subsequently projected onto each of the camera views, and serve as seeds to ', 'for further occlusion handling, refinement and candidate object proposal ', 'generation. We cull out (i) small candidates whose window size is less than ', '20x20 px, (ii) occluding candidates by estimating their median depth from the ', 'reconstruction, to avoid mis-identification and (iii) overlapping candidates ', 'with an IoU threshold of 0.5, to avoid redundant proposals. The filtered set of ', 'windows are subsequently considered as candidates for the classification process ', 'downstream. Figure~\\\\ref{fig:multi-view-objectness} illustrates the different ', 'steps described in this section. ', ' ', ' ', '% With this in mind, we perform spatial clustering on the scale-ambiguous map ', '% using a na\\\\\"{\\\\i}ve density threshold that appropriately segments out candidate ', '% object proposals in the reconstructed map, and simultaneously rejects outliers in ', '% the form of spurious points or edges that are inconsistent across multiple ', '% viewpoints. These object candidates are subsequently projected onto each of the ', '% camera views and further culled on a frame-by-frame basis based on their window ', '% size. We cull out proposal regions that occupy less than 20px-wide ', '% windows in the image, and the resulting windows are considered as candidates for ', '% the classification process downstream. } ', ' ', ' ', '% Objectness generally refers to a measure of how ', '% likely a region in an image may contain an object of any category. Most ', '% state-of-the-art recognition systems require category-specific detectors that ', '% activate over many windows or bounding boxes in a sliding window ', '% fashion~\\\\cite{felzenszwalb2010object, dalal2005histograms}.  ', ' ', ' ', ' ', '\\\\subsection{State-of-the-art Bag-of-Visual-Words with Object Proposals} ', '\\\\label{subsec:proc-vlad-flair} ', ' ', ' ', 'Given the object proposals computed using the reconstructed scale-ambiguous map, ', 'we now direct our attention to describing these proposal regions.  ', ' ', '% Most recently, ', '% these object proposal strategies have opened the door to the use of ', '% bag-of-visual-words-based (BoVW) approaches that are known to be semantically ', '% richer in their representation. We take a similar approach:  ', ' ', '% ', '% VLAD FLAIR figure ', '% ', '\\\\begin{figure*}[!t] ', '    \\\\centering ', '    \\\\includegraphics[width=2\\\\columnwidth]{graphics/bow/vlad-flair.png} ', '    \\\\caption{Various steps involved in the feature extraction ', 'procedure. Features that are densely sampled from the image are subsequently ', 'used to describe the multi-view object proposals using FLAIR. Each proposal is ', 'described with multiple ([1x1], [2x2], [4x4]) spatial levels/bins via quick ', 'table lookups in the integral VLAD histograms (through FLAIR). The resulting ', 'histogram $\\\\Psi$ (after concatenation) is used to describe ', 'the object contained in the bounding box. Figure is best viewed in electronic form.} ', '    \\\\label{fig:feature-extraction} ', '\\\\end{figure*} ', ' ', '\\\\textbf{Dense BoVW with VLAD} Given an input image ', 'and candidate object proposals, we first densely sample the image, describing ', 'each of the samples with SIFT + RGB color values, $\\\\Phi_{SIFT+RGB} \\\\in ', '\\\\mathbb{R}^{131}$ i.e. Dense SIFT~(128-D) + RGB(3-D). Features are extracted with a ', 'step size of 4 pixels, and at 4 different pyramid scales with a pyramid scale ', 'factor of $\\\\sqrt[]{2}$. The resulting description is then reduced to a ', '80-dimensional vector via PCA, called PCA-SIFT $\\\\Phi \\\\in \\\\mathbb{R}^{80}$. A ', 'vocabulary $V \\\\in \\\\mathbb{R}^{K\\\\times80}$ of size $K=64$ is created via ', '$k$-means, using the descriptions extracted from a shuffled subset of the training data, ', 'as done in classical bag-of-visual-words approaches. In classical BoVW, this ', 'vocabulary can be used to encode each of the original SIFT+RGB descriptions in ', 'an image into a histogram of occurrences of codewords, which in turn provides a ', 'compact description of the original image. Recently, however, more ', 'descriptive encodings such as VLAD~\\\\cite{jegou2010aggregating} and Fisher ', 'Vectors~\\\\cite{perronnin2010improving} have been shown to outperform classical ', 'BoVW ', 'approaches~\\\\cite{jegou2010aggregating,perronnin2010improving,chatfield2011devil}. Consequently, ', 'we chose to describe the features using VLAD as it provides equally as strong performance ', 'with slightly reduced computation time as compared to Fisher Vectors.  ', ' ', 'For each of the bounding boxes, the un-normalized VLAD $\\\\Psi \\\\in ', '\\\\mathbb{R}^{KD}$ description is computed ', 'by aggregating the residuals of each of the descriptions $\\\\Phi$ (enclosed within the ', 'bounding box) from their vector-quantized centers in the vocabulary, thereby ', 'determining its first order moment (Eq.~\\\\ref{eqn:vlad-first-order}). \\\\vspace{-1mm} ', '\\\\begin{align} ', 'v_k = \\\\sum_{x_i:NN(x_i)=\\\\mu_k} x_i - \\\\mu_k ', '\\\\label{eqn:vlad-first-order} ', '\\\\end{align}  ', 'The ', 'description is then normalized using signed-square-rooting (SSR) or commonly ', 'known as power normalization (Eq.~\\\\ref{eq:signed-square-rooting}) with ', '$\\\\alpha=0.5$, followed by L2 normalization, for improved recognition performance ', 'as noted in~\\\\cite{arandjelovic2013all}. ', '\\\\begin{align} f(z) = sign(z)\\\\vert z \\\\vert^\\\\alpha \\\\quad \\\\text{where} \\\\quad 0 \\\\leq ', '\\\\alpha \\\\leq 1 ', '\\\\label{eq:signed-square-rooting} ', '\\\\end{align} Additional descriptions for each bounding region are constructed for ', '3 different spatial bin levels or subdivisions as noted in ', '~\\\\cite{lazebnik2006beyond} (1x1, 2x2 and 4x4, 21 total subdivisions $S$), and ', 'stacked together to obtain the feature vector $\\\\Psi = \\\\begin{bmatrix} ', '\\\\hdots \\\\textbf{v}_s \\\\hdots \\\\end{bmatrix} \\\\in  ', '\\\\mathbb{R}^{KDS}$ that appropriately describes the ', 'specific object contained within the candidate object proposal/bounding box.\\\\vspace{2mm} ', ' ', '\\\\textbf{Efficient Feature Encoding with FLAIR} While it may be practical to ', 'describe a few object proposals in the scene with these encoding methods, it can ', 'be highly impractical to do so as the number of object proposals grows. To this ', 'end, \\\\citet{van2014fisher} introduced FLAIR - an encoding mechanism that ', 'utilizes summed-area tables of histograms to enable fast descriptions for ', 'arbitrarily many boxes in the image. By constructing integral histograms for ', 'each code in the codebook, the histograms or descriptions for an arbitrary ', 'number of boxes $B$ can be computed independent of their area. As shown ', 'in~\\\\cite{van2014fisher}, these descriptions can also be extended to the VLAD ', 'encoding technique. Additionally, FLAIR affords performing spatial pyramid ', 'binning rather naturally, with only requiring a few additional table look-ups, ', 'while being independent of the area of $B$. We refer the reader to ', 'Figure~\\\\ref{fig:feature-extraction} for an illustration of the steps involved in ', 'describing these candidate object proposals.\\\\vspace{2mm} ', ' ', '\\\\textbf{Multi-class histogram classification} Given training examples, $(x_1, y_1),\\\\ldots,(x_n, y_n)$ where $x_i\\\\in\\\\mathbb{R}^{KDS}$ are the VLAD descriptions  ', 'and $y_i\\\\in\\\\{1,\\\\ldots, \\\\mathcal{C}\\\\}$ are the ground truth target labels, ', 'we train a linear classifier using Stochastic Gradient Descent (SGD), given by: ', '\\\\begin{align}  ', 'E(w) = \\\\frac{1}{n}\\\\sum_{i=1}^{n} L(y_i, f(x_i)) + \\\\alpha R(w) ', '\\\\label{eq:sgd-classifier} ', '\\\\end{align} \\\\vspace{-1mm} where $L(y_i, f(x_i)) = ', '\\\\text{log}\\\\Big(1+\\\\text{exp}(-y_i\\\\textbf{w}^T\\\\textbf{x}_i)\\\\Big)$ is the logistic ', 'loss function, $R(w) = \\\\frac{1}{2} \\\\sum_{i=1}^{n} \\\\textbf{w}^T\\\\textbf{w}$ is the L2-regularization term that penalizes model complexity, and $\\\\alpha ', '> 0$ is a non-negative hyperparameter that adjusts the L2 regularization. A ', 'one-versus-all strategy is taken to extend the classifiers to multi-class ', 'categorization. For hard-negative mining, we follow~\\\\cite{van2014fisher} ', 'closely, bootstrapping additional examples from wrongly classified negatives for ', '2 hard-negative mining epochs. ', ' ', ' ', ' ', ' ', '% Frame-based recognition ', '% ', '\\\\begin{figure*}[!tp] ', '    \\\\centering ', '    \\\\includegraphics[width=2\\\\columnwidth]{graphics/frame_detect/frame_detect.png} ', '    \\\\caption{Illustration of \\\\textbf{per-frame} detection results provided by our object ', '      recognition system that is \\\\textit{intentionally SLAM-oblivious} (for ', '      comparison purposes only). Object recognition evidence is ', '      not aggregated across all frames, and detections are performed on a ', '      frame-by-frame basis. Only detections having corresponding ground truth ', '      labels are shown. Figure is best viewed in electronic form. } ', '    \\\\label{fig:frame-detection-results} ', '\\\\end{figure*} ', '% ', ' ', '% SLAM-based recognition ', '% ', '\\\\begin{figure*}[t] ', '    \\\\centering ', '    \\\\includegraphics[width=2\\\\columnwidth]{graphics/slam_detect/slam_detect-2.png} ', '    \\\\caption{Illustration of the recognition capabilities of our proposed ', '      SLAM-aware object recognition system. Each of the object categories are detected ', '      every frame, and their evidence is aggregated across the entire sequence through ', '      the set of object hypothesis. In frame-based object recognition, predictions are ', '      made on an individual image basis (shown in gray).  In SLAM-aware recognition, ', '      the predictions are aggregated across all frames in the image sequence to ', '      provide robust recognition performance. The green boxes indicate correctly ', '      classified object labels, and the gray boxes indicate background object ', '      labels. Figure is best viewed in electronic form.} ', '    \\\\label{fig:slam-detection-results-1} ', '\\\\end{figure*} ', ' ', ' ', '\\\\subsection{Multi-view Object Recognition} ', '\\\\label{sec:proc-multiview-recognition} We start with the ORB-SLAM-based ', 'semi-dense mapping solution, that feeds a continuous image stream, in order to ', 'recover a scale-ambiguous map~$\\\\mathcal{M}$, keyframes~$\\\\mathcal{K}$, and poses ', '${\\\\xi}$ corresponding to each of the frames in the input image stream. The ', 'resulting scale-ambiguous reconstruction provides a strong indicator of object ', 'presence in the environment, that we use to over-segment into several object ', 'seeds $o \\\\in \\\\{1, \\\\dots, \\\\mathcal{O}\\\\}$.  These object seeds are projected back ', 'in to each of the individual frames using the known projection matrix, derived ', 'from its corresponding viewpoint $\\\\xi_i$. The median depth estimates of each of ', 'the seeds are estimated in order to appropriately project non-occluding object ', 'proposals back in to corresponding viewpoint, using a depth buffer. Using these ', 'as candidate object proposals, we evaluate our detector on each of the ', '$\\\\mathcal{O}$ object clusters, per image, providing probability estimates of ', 'belonging to one of the $\\\\mathcal{C}$ object classes or categories. Thus, the ', 'maximum-likelihood estimate of the object $o \\\\in \\\\mathcal{O}$ can be formalized ', 'as maximizing the data-likelihood term for all observable viewpoints (assuming ', 'uniform prior across the $\\\\mathcal{C}$ classes): ', '\\\\begin{align} ', '\\\\hat{y}^{MLE} = \\\\underset{y\\\\in\\\\{1,\\\\dots,\\\\vert\\\\mathcal{C}\\\\vert\\\\}}{\\\\mathrm{argmax}}~p(\\\\mathcal{D}_{o}\\\\mid y) \\\\quad \\\\forall~o \\\\in \\\\mathcal{O} ', '\\\\label{eq:multi-view-evidence} ', '\\\\end{align} ', 'where $y\\\\in\\\\{1,\\\\dots,\\\\vert\\\\mathcal{C}\\\\vert\\\\}$ are ', 'the class labels, $\\\\mathcal{D}_{o} = \\\\{x_{1}, \\\\dots, x_{N}\\\\}_{o}$ is the data observed of the ', 'object cluster $o \\\\in \\\\mathcal{O}$ across $N$ observable viewpoints. In our ', 'case, $\\\\mathcal{D}_{o}$ refers to the bounding box of ', 'the $o^{th}$ cluster, projected onto each of the $N$ ', 'observable viewpoints. Assuming the individual features in $\\\\mathcal{D}_{o}$ are conditionally ', 'independent given the class label $y$, the maximum-likelihood estimate (MLE) ', 'factorizes to: \\\\vspace{-2mm} ', '\\\\begin{align}  ', '\\\\hat{y}^{MLE} &= \\\\underset{y\\\\in\\\\{1,\\\\dots,\\\\vert\\\\mathcal{C}\\\\vert\\\\}}{\\\\mathrm{argmax}} \\\\prod_{n=1}^N p(x_n\\\\mid y)\\\\\\\\  ', '                   &= \\\\underset{y\\\\in\\\\{1,\\\\dots,\\\\vert\\\\mathcal{C}\\\\vert\\\\}}{\\\\mathrm{argmax}} \\\\sum_{n=1}^N \\\\log p(x_n\\\\mid y) ', '\\\\label{eq:multi-view-evidence-2} ', '\\\\end{align}  ', 'Thus the MLE of an object cluster $o$ belonging to one of the ', '$\\\\mathcal{C}$ classes, is the class that corresponds to having the highest of ', 'the sum of the log-likelihoods of their individual ', 'class probabilities estimated for each of the $N$ observable viewpoints. ', ' ', ' ', '% \\\\input{tex/sec-proc-implementation} ', ' ', '% ========================================= ', ' ', ' ', '% \\\\subsection{Post-Processing} ', '% \\\\begin{align} ', '% v_k &= \\\\frac{1}{M\\\\sqrt{\\\\pi_k}} \\\\sum_{i=1}^M \\\\gamma_k(x_i)~\\\\frac{x_i - ', '%   \\\\mu_k}{\\\\sigma_k}\\\\\\\\ ', '% u_k &= \\\\frac{1}{M\\\\sqrt{2\\\\pi_k}} \\\\sum_{i=1}^M \\\\gamma_k(x_i)~\\\\bigg(\\\\frac{x_i - ', '%   \\\\mu_k}{\\\\sigma_k} - 1\\\\bigg)^2 ', '% \\\\label{eqn:fisher-second-order} ', '% \\\\end{align} ', '% where $\\\\gamma_k(x_i)$. ', ' ', '% \\\\textbf{Efficient Additive Kernels}:  ', ' ', '% \\\\textbf{Linear kernel}: ', '% \\\\begin{align} ', '% K(\\\\textbf{x},\\\\textbf{y}) = \\\\sum_{l=1}^D \\\\textbf{x}_l \\\\textbf{y}_l ', '% \\\\label{eqn:linear-kernel} ', '% \\\\end{align} ', ' ', ' ', '% \\\\section{SLAM-Aware Object Recognition} ', '% \\\\label{sec:proc-spatially-aware-object-recognition} ', ' ', '% \\\\subsection{SLAM-Backed Robust Recognition} ', '% \\\\label{subsec:proc-slam-backed-robust-recognition} ', ' ', '% \\\\subsection{Improved Efficiency via Active Recognition} ', '% \\\\label{subsec:proc-improved-efficiency-via-active-recognition} ', ' ', '% \\\\subsection{Recognition-Aware SLAM and SLAM-Aware Recognition} ', '% \\\\label{subsec:proc-recog-slam} ', ' ', ' ', ' ', ' ', ' ', '\\\\section{Experiments} ', '\\\\label{sec:experiments} ', 'In this section, we evaluate the proposed SLAM-aware object ', 'recognition method. In our experiments, we extensively evaluate our SLAM-aware ', 'recognition system on the popular UW RGB-D ', 'Dataset (v2)\\\\cite{lai2011large,laiunsupervised}. We compare against the current ', 'state-of-the-art solution proposed by~\\\\citet{lai2012detection}, that utilize ', 'full map and camera location information for improved ', 'recognition performance. The UW RGB-D dataset contains a total 51 object ', 'categories, however, in order to maintain a fair comparison, we consider the ', 'same set of 5 objects as noted in~\\\\cite{lai2012detection}. In experiment 3, we ', 'propose scalable recognition solutions, increasing the number of objects ', 'considered to all 51 object categories in the UW RGB-D Dataset.\\\\vspace{1mm} ', ' ', '% \\\\input{tex/sec-results-baseline} ', ' ', '\\\\textbf{Experiment 1: SLAM-Aware Object Recognition Performance Evaluation} ', '\\\\label{experiment-1} We train and evaluate our system on the UW RGB-D Scene ', 'Dataset~\\\\cite{lai2011large,laiunsupervised}, providing mean-Average Precision ', '(mAP) estimates (see Table~\\\\ref{tab:mAP-SLAM}) for the object recognition task ', 'and compare against existing methods~\\\\cite{lai2012detection}. We split our ', 'experiments into two categories: \\\\vspace{1mm}  ', ' ', '\\\\input{results/mAP-slam} ', ' ', '\\\\textit{(i) Single-View recognition performance: } First, we evaluate the ', 'recognition performance of our proposed system on each of the scenes in the ', 'UW-RGB-D Scene Dataset on a per-frame basis, detecting and classifying objects ', 'that occur every 5 frames in each scene (as done ', 'in~\\\\cite{lai2012detection}). Each object category is trained from images in the ', 'Object Dataset, that includes several viewpoints of object instances with their ', 'corresponding mask, and category information. Using training parameters ', 'identical to the previous experiment, we achieve a performance of ', '81.5 mAP as compared to the detector performance of 61.7 mAP ', 'reported in~\\\\cite{lai2012detection}. Recognition is done on a per-image ', 'basis, and averaged across all test images for reporting. ', 'Figure~\\\\ref{fig:frame-detection-results} shows the recognition results of our ', 'system on a per-frame basis. We ignore regions labeled as background in the ', 'figure for clarity and only report the correct and incorrect predictions in ', 'green and red respectively. \\\\vspace{1mm} ', ' ', '\\\\textit{(ii) Multi-View recognition performance: } In this section, we ', 'investigate the performance of a SLAM-aware object recognition system. We ', 'compare this to a SLAM-oblivious object detector described previously, and ', 'evaluate using ground truth provided. Using the poses $\\\\xi$ and reconstructed ', 'map $\\\\mathcal{M}$, multi-view object candidates are proposed and projected onto ', 'each of the images for each scene sequence. Using the candidates provided as ', 'input to the recognition system, the system predicts the likelihood and ', 'corresponding category of an object (including background) contained in a ', 'candidate bounding box. For each of the objects $o \\\\in \\\\mathcal{O}$ proposed, ', 'the summed log-likelihood is computed (as in Eqn.~\\\\ref{eq:multi-view-evidence}) ', 'to estimate the most likely object category over all the images for a particular ', 'scene sequence. We achieve 89.8 mAP recognition performance on the 5 objects in ', 'each of the scenes in~\\\\cite{laiunsupervised} that was successfully reconstructed ', 'by the ORB-SLAM-based semi-dense mapping ', 'system. Figures~\\\\ref{fig:intro-fig},~\\\\ref{fig:multi-view-objectness}, ', '~\\\\ref{fig:slam-detection-results-1} and~\\\\ref{fig:more-slam-detect} illustrate ', 'the capabilities of the proposed system in providing robust recognition ', 'performance by taking advantage of the monocular visual ', 'SLAM-backend. Figure~\\\\ref{fig:mAP-SLAM} illustrates the average precision-recall ', 'performance on the UW RGB-D dataset, comparing the classical frame-based and our ', 'SLAM-aware approach. As expected, with additional object viewpoints, our ', 'proposed SLAM-aware solution predicts with improved precision and recall. In ', 'comparison to that of HMP2D+3D~\\\\cite{laiunsupervised}, they achieve only ', 'slightly higher overall recognition performance of 90.9 mAP, as their ', 'recognition pipeline takes advantage of the RGB and depth input to improve ', 'overall scene reconstruction. We do note that while we perform comparably with ', 'HMP2D+3D~\\\\cite{laiunsupervised}, our BoVW+FLAIR architecture allows our system ', 'to scale to a large number of object categories with \\\\textit{near-constant ', 'run-time}. We investigate the run-time performance and scalability concerns ', 'further in Experiment 3.  \\\\vspace{1mm} ', ' ', ' ', '% ', '% \\\\begin{figure}[!h] ', '%     \\\\centering ', '%     % {\\\\setlength{\\\\tabcolsep}{1mm} ', '%     \\\\begin{tabular}{cc} ', '%       \\\\hspace{-2mm} ', '%       \\\\includegraphics[width=0.5\\\\columnwidth]{results/frame_slam_average_pr.pdf} ', '%       \\\\hspace{-2mm} ', '%       \\\\includegraphics[width=0.5\\\\columnwidth]{results/multiview_pr.pdf} ', '%     \\\\end{tabular} ', '%     \\\\caption{Precision-Recall curves for frame-based detection compared to that ', '%       of our proposed SLAM-aware approach. \\\\textbf{Left:} PR curves indicating recognition ', '%       performance for each object category using our multi-view object proposals, and ', '%       VLAD+FLAIR feature encoding on the UW-RGBD dataset. \\\\textbf{Right:} ', '%       Performance comparison via precision-recall for the frame-based vs. SLAM-aware object ', '%       recognition. As expected, the performance of our proposed SLAM-aware ', '%       solution increases with more recognition evidence accumulated across ', '%       multiple viewpoints. } ', '%     \\\\label{fig:slam-detection-pr} ', '%   % } ', '% \\\\end{figure} ', ' ', ' ', '% ', '% ', ' ', '% % ', '% \\\\begin{figure*}[!t] ', '%     \\\\centering ', '%     \\\\includegraphics[width=2\\\\columnwidth]{graphics/slam_detect/slam_detect.png} ', '%     \\\\caption{More illustrations of strong performance results achieved via our ', '%       SLAM-aware recognition system. Figure is best viewed in electronic form.} ', '%     \\\\label{fig:slam-detection-results-2} ', '% \\\\end{figure*} ', '% % ', '% % ', ' ', ' ', '\\\\begin{figure}[b] ', '  \\\\centering ', '  \\\\vspace{2mm} ', '    \\\\includegraphics[width=0.95\\\\columnwidth]{graphics/objectness/objectness.pdf}  ', '    \\\\caption{\\\\textit{Varying number of proposals:} We experiment with varied ', '      number of bounding boxes for the BING object proposal method, and compare ', '      against our multi-view object proposal method that uses considerably fewer ', '      number of bounding boxes to get similar or better recall rates. The numbers next ', '      to the label indicate the average number of windows proposed in the image. } ', '    \\\\label{fig:objectness-performance} ', '\\\\end{figure} ', ' ', '\\\\textbf{Experiment 2: Multi-View Objectness} ', '\\\\label{subsec:slam-driven-objectness} In this experiment, we investigate the ', 'effectiveness of our multi-view object proposal method in identifying ', 'category-independent objects in a continuous video stream. We compare the recall ', 'of our object proposal method with the recently introduced ', 'BING~\\\\cite{cheng2014bing} object proposal technique, whose performance in ', 'detection rate (DR) and run-time claim to be promising. We compare against the ', 'BING method, varying the number of proposed object candidates by picking ', 'proposals in descending order of their objectness ', 'score. Figure~\\\\ref{fig:objectness-performance} compares the overall performance ', 'of our multi-view object proposal method that achieves better recall ', 'rates, for a particular IoU threshold with ', 'considerably fewer object proposals. The results provided are evaluated ', 'on all the scenes provided in the UW-RGB-D dataset ', '(v2)~\\\\cite{laiunsupervised}.  ', '% \\\\vspace{2mm} ', ' ', '% Some of results are illustrated in Figure~\\\\ref{fig:multi-view-objectness-2}.\\\\vspace{1mm} ', ' ', ' ', '\\\\textbf{Experiment 3: Scalable recognition and run-time evaluation} ', '\\\\label{subsec:results-run-time-perf} In this section, we investigate the ', 'run-time performance of computing VLAD with integral histograms (FLAIR) for our ', 'system and compare against previously proposed ', 'approaches~\\\\cite{van2014fisher,lai2012detection}. We measure the average speed ', 'for feature-extraction (Dense-SIFT) and feature-encoding (VLAD) as they take up ', 'over 95\\\\% of the overall compute time. All experiments were conducted with a ', 'single-thread on an Intel Core-i7-3920XM (2.9GHz).  ', ' ', '% The runtimes indicated in the ', '% FLAIR feature encoding step includes the vector-quantization step to find the closest ', '% code words. ', ' ', '% We experiment with ', '% varied dense sampling step sizes of 2, 4, 6, 8 and 10 px, and several several ', '% descriptor types including SIFT~\\\\cite{lowe2004distinctive}, ', \"% OpponentSIFT\\\\footnote{The ``Opponent'' prefix indicates that the descriptors \", '% were extracted across 3 channels in the Opponent color space.}, ', '% ORB~\\\\cite{rublee2011orb}, and OpponentORB, and report the run-times for ', '% each.  ', '% See ', '% figure~\\\\ref{fig:runtime-performance} for runtime evaluation experiment ', '% description. ', ' ', '% We ', '% maintain a fixed vocabulary size of $K=64$, and extract features densely over 4 ', '% pyramid scales, with a scale factor of $\\\\sqrt[]{2}$. We also fix the spatial ', '% pyramids constant, subdividing each window/bounding-box to 1x1, 2x2 and 4x4 ', '% grids.  ', ' ', '\\\\input{results/runtime.tex} ', ' ', '\\\\citet{van2014fisher} reports that the overall feature extraction ', 'and encoding takes 5.15s (VQ 0.55s, FLAIR construction 0.6s, VLAD+FLAIR 4.0s) ', 'per image, with the following parameters (2px step size, 3 Pyr. Scales, ', '[1x1],~[4x4] spatial pyramid bins). With significantly fewer candidate ', 'proposals, and careful implementation, our system is able to achieve the same ', '(with 4px step size) in ', 'approximately 1.6s. With reference to~\\\\cite{lai2012detection}, where the ', 'run-time performance of the sliding-window approach is directly proportional to ', 'the number of object categories detectable, the authors report an overall ', 'run-time of 1.8s for 5 object categories. However, scaling up their detection to ', 'larger number of objects would imply costly runtimes, making it highly ', 'impractical for real-time purposes. The run-time of our approach (based ', 'on~\\\\cite{van2014fisher}), on the other hand, is scalable to a larger number of ', 'object categories, making it a strong contender for real-time recognition ', 'systems. We summarize the run-times of our approach compared to that ', 'of~\\\\cite{lai2012detection} and~\\\\cite{laiunsupervised} in Table~\\\\ref{table:runtime}. \\\\vspace{1mm} ', ' ', '% Additionally, as seen ', '% in Figure~\\\\ref{fig:runtime-performance}, we also ', '% note that it may be especially advantageous for robots to \\\\textit{be able to vary ', '% parameters such as step size, and descriptor type that directly affect the ', '% run-time performance of the system, at the cost of accuracy, depending on the ', '% task at hand}.  ', ' ', '% \\\\begin{figure}[h] ', '%   \\\\centering ', '%     \\\\includegraphics[width=\\\\columnwidth]{graphics/runtime/runtime.pdf}  ', '%     \\\\caption{\\\\textit{Varying step size and descriptors:} Plots illustrating the ', '%       run-time performance of our system with varied step size and descriptors} ', '%     \\\\label{fig:runtime-performance} ', '% \\\\end{figure} ', ' ', ' ', '% RGBD mapping and HOG based feature extraction ', '% Running time. The RGB-D Mapping algorithm [18] used ', '% for scene reconstruction runs in real-time for our Kinect ', '% videos, which were collected at 15-20 Hz. We evaluate object ', '% detectors on every 10th frame, or every 0.5 seconds. Our ', '% current single-threaded MATLAB implementation is not yet ', '% real-time, requiring 4 seconds to process each frame. Vox- ', '% elization and graph cut inference take negligible time. The ', '% overwhelming majority of computation is spent on feature ', '% extraction and sliding window classification, each taking ', '% around 1.8 seconds. ', ' ', ' ', ' ', '% % Cloud Objectness figure ', '% % ', '% \\\\begin{figure*}[!t] ', '%     \\\\centering ', '%     \\\\includegraphics[width=2\\\\columnwidth]{graphics/objectness/labeled/scene-labels.png} ', '%     \\\\caption{More illustrations of the multi-view object proposals method. Each ', '%       scene is reconstructed in a semi-dense fashion using our ORB-SLAM-based ', '%       semi-dense mapping solution, ', '%       and over-segmented using a multi-scaled spatial density segmentation. 4th and 5th ', '%       columns on the first 2 rows: The resulting ', '%       over-segmentations are used as seeds (in red) to propose candidate windows in the ', '%       image (to cover object parts that failed to reconstruct). Randomized ', '%       Prim~\\\\cite{manen2013prime} is used to propose 3-4 candidate windows ', '%       corresponding to the seed. Last row: Few examples of top 3 candidate ', '%       windows proposed. The last two columns show failures in the proposal generation} ', '%     \\\\label{fig:multi-view-objectness-2} ', '% \\\\end{figure*} ', '% % ', ' ', ' ', ' ', ' ', '% \\\\subsection{Scalable Recognition} ', '% \\\\label{subsec:results-scalable-recog} ', '% \\\\textbf{Experiment 3: Recognition performance with varied number of object categories } ', ' ', '% \\\\subsection{Efficient-Active Recognition} ', '% \\\\label{subsec:results-active-recog} ', '% \\\\textbf{Experiment 4: Improved recognition efficiency} ', ' ', ' ', ' ', ' ', ' ', '\\\\begin{figure}[h] ', '  \\\\centering ', '    \\\\includegraphics[width=\\\\columnwidth]{graphics/slam_detect/slam_detect-4.png}  ', '    \\\\caption{More illustrations of the superior performance of the ', '        SLAM-aware object recognition in scenarios of ambiguity and ', '        occlusions. The coffee mug is misidentified as a soda can, and the cap ', '        in the bottom row is occluded by the cereal box.} ', '    \\\\label{fig:more-slam-detect} ', '\\\\end{figure} ', ' ', '\\\\textbf{Discussion and Future Work} While there are benefits to running a ', 'monocular visual SLAM-backend for recognition purposes, the inter-dependence of ', 'the recognition system on this backend makes it vulnerable to the same ', 'robustness concerns that pertain to monocular visual SLAM. In our experiments, ', 'we noticed inadequacies in the semi-dense vSLAM implementation that failed to ', 'reconstruct the scene on few occasions. To further emphasize recognition ', 'scalability, we are actively collecting a larger scaled dataset (in increased ', 'map area, and number of objects) to show the extent of capabilities of the ', 'proposed system. Furthermore, we realize the importance of real-time ', 'capabilities of such recognition systems, and intend to generalize the ', 'architecture to a streaming approach in the near future. We also hope to release ', 'the source code for our proposed method, allowing scalable and customizable ', 'training with fast run-time performance during live operation. ', ' ', '% We continue to ', '% perform evaluations on datasets that present large variability in object ', '% categories and viewpoints.  ', ' ', '% We do note however, that the LSD-SLAM ', '% system used was not robust to the aggressive motions in this specific ', '% dataset.  ', ' ', '\\\\section{Conclusion} In this work, we develop a SLAM-aware object-recognition ', 'system, that is able to provide robust and scalable recognition performance as ', 'compared to classical SLAM-oblivious recognition methods. We leverage some of ', 'the recent advancements in semi-dense monocular SLAM to propose objects in the ', 'environment, and incorporate efficient feature encoding techniques to provide an ', 'improved object recognition solution whose run-time is \\\\textit{nearly-constant} ', 'to the number of objects identifiable by the system. Through various ', 'evaluations, we show that our SLAM-aware monocular recognition solution is ', 'competitive to current state-of-the-art in the {RGB-D} object recognition ', 'literature. We believe that robots equipped with such a monocular system will be ', 'able to robustly recognize and accordingly act on objects in their environment, ', 'in spite of object clutter and recognition ambiguity inherent from certain ', 'object viewpoint angles. ', '~ ', ' ', ' ', ' ', '% UNCOMMENT FOR CAMERA-READY SUBMISSION ', '\\\\section*{Acknowledgments} This work was funded by the Office of Naval Research ', 'under grants MURI N00014-10-1-0936, N00014-11-1-0688 and N00014-13-1-0588 and by ', 'the National Science Foundation under Award IIS-1318392. We would like to thank ', 'the authors of ORB-SLAM and LSD-SLAM for providing source code of their work, ', 'and the authors of the UW-RGB-D Dataset~\\\\cite{lai2012detection,laiunsupervised} ', 'for their considerable efforts in collecting, annotating and developing ', 'benchmarks for the dataset. \\\\pagebreak ', ' ', ' ', '% REFERENCES ', '% ----------------------------------------------------------------------- ', '%% Use plainnat to work nicely with natbib.  ', ' ', '\\\\bibliographystyle{abbrvnat_ordered} % plainnat ', '{\\\\small ', '\\\\bibliography{tex/references} ', '} ', ' ', '% END ', '% ----------------------------------------------------------------------- ', '\\\\end{document} ', ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "print content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'introduction': 'introduction'}\n",
      "{'introduction': 'introduction', 'related work': 'related work'}\n",
      "{'introduction': 'introduction', 'related work': 'related work', 'monocular slam supported \\\\\\\\object recognition': 'monocular slam supported \\\\\\\\object recognition'}\n",
      "{'introduction': 'introduction', 'related work': 'related work', 'slam-aware object recognition': 'slam-aware object recognition', 'monocular slam supported \\\\\\\\object recognition': 'monocular slam supported \\\\\\\\object recognition'}\n",
      "{'introduction': 'introduction', 'related work': 'related work', 'experiments': 'experiments', 'slam-aware object recognition': 'slam-aware object recognition', 'monocular slam supported \\\\\\\\object recognition': 'monocular slam supported \\\\\\\\object recognition'}\n",
      "{'slam-aware object recognition': 'slam-aware object recognition', 'introduction': 'introduction', 'related work': 'related work', 'experiments': 'experiments', 'monocular slam supported \\\\\\\\object recognition': 'monocular slam supported \\\\\\\\object recognition', 'conclusion': 'conclusion'}\n",
      "{'slam-aware object recognition': 'slam-aware object recognition', 'introduction': 'introduction', 'related work': 'related work', 'experiments': 'experiments', 'monocular slam supported \\\\\\\\object recognition': 'monocular slam supported \\\\\\\\object recognition', 'acknowledgments': 'acknowledgments', 'conclusion': 'conclusion'}\n"
     ]
    }
   ],
   "source": [
    "names = {}\n",
    "for line in content:\n",
    "    temp = []\n",
    "    \n",
    "    # find sections\n",
    "    val =  line.find('\\section')\n",
    "    if val != -1:\n",
    "#         print val\n",
    "#         print line[val:]\n",
    "        # find the names of the sections\n",
    "        flag = 0\n",
    "        for char in line:\n",
    "            if char == '}':\n",
    "                flag = 0\n",
    "            if flag == 1:\n",
    "                temp.append(char)\n",
    "            if char == '{':\n",
    "                flag = 1\n",
    "#             print temp\n",
    "            name = ''.join(temp)\n",
    "#             print name\n",
    "        \n",
    "        # save names of the sections into dictionary\n",
    "        names[name.lower()] = name.lower()\n",
    "        print names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZz\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "slam-aware object recognition\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZz\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "introduction\n",
      " Object recognition is a vital component in a robot's repertoire of skills. Traditional object recognition methods have focused on improving recognition performance (Precision-Recall, or mean Average-Precision) on specific datasets~\\cite{everingham2010pascal, ILSVRC15}. While these datasets provide sufficient variability in object categories and instances, the training data mostly consists of images of arbitrarily picked scenes and/or objects. Robots, on the other hand, perceive their environment as a continuous image stream, observing the same object several times, and from multiple viewpoints, as it constantly moves around in its immediate environment. As a result, object detection and recognition can be further bolstered if the robot were capable of simultaneously localizing itself and mapping (SLAM) its immediate environment - by integrating object detection evidences across multiple views.       \\begin{figure}[!t]   \\centering   \\begin{tabular}{c}     \\includegraphics[width=\\columnwidth]{graphics/slam_detect/intro.pdf}   \\end{tabular}   \\caption{The proposed SLAM-aware object recognition system is able to robustly     localize and recognize several objects in the scene, aggregating detection evidence across multiple views. Annotations in white are provided for clarity and are actual predictions     proposed by our system.}   \\label{fig:intro-fig} \\end{figure}     We refer to a ``SLAM-aware'' system as - one that has access to the map of its observable surroundings as it builds it incrementally and the location of its camera at any point in time. This is in contrast to classical recognition systems that are ``SLAM-oblivious'' - those that detect and recognize objects on a frame-by-frame basis without being cognizant of the map of its environment, the location of its camera, or that objects may be situated within these maps. \\textit{In this paper, we develop the ability for a SLAM-aware system to robustly recognize objects in its environment, using an RGB camera as its only sensory input} (Figure~\\ref{fig:intro-fig}).  We make the following contributions towards this end: Using state-of-the-art semi-dense map reconstruction techniques in monocular visual SLAM as pre-processed input, we introduce the capability to propose multi-view consistent object candidates, as the camera observes instances of objects across several disparate viewpoints. Leveraging this object proposal method, we incorporate some of the recent advancements in bag-of-visual-words-based (BoVW) object classification~\\cite{jegou2010aggregating,delhumeau2013revisiting,arandjelovic2013all} and efficient box-encoding methods~\\cite{van2014fisher} to enable strong recognition performance.  The integration of this system with a monocular visual-SLAM (vSLAM) back-end also enables us to take advantage of both the reconstructed map and camera location to significantly bolster recognition performance. Additionally, our system design allows the run-time performance to be scalable to a larger number of object categories, with near-constant run-time for most practical object recognition tasks.   \\begin{figure*}[!t]     \\centering     % \\def\\svgwidth{\\columnwidth}     % \\input{graphics/slam_detect/pipeline.pdf_tex}     \\includegraphics[width=2\\columnwidth]{graphics/slam_detect/pipeline.pdf}     \\caption{Outline of the SLAM-aware object recognition pipeline. Given an input RGB image stream $\\mathcal{I}$, we first reconstruct the scene in a semi-dense fashion using an existing monocular visual-SLAM implementation (ORB-SLAM) with a semi-dense depth estimator, and subsequently extract relevant map $\\mathcal{M}$, keyframe $\\mathcal{K}$ and pose information $\\xi$. We perform multi-scale density-based segmentation on the reconstructed scene to obtain object proposals $\\mathcal{O}$ that are consistent across multiple views. On each of the images in the input RGB image stream $\\mathcal{I}$, we compute Dense-SIFT ($\\mathbb{R}^{128}$) + RGB ($\\mathbb{R}^{3}$) and reduce it to $\\Phi \\in \\mathbb{R}^{80}$ via PCA. The features $\\Phi$ are then used to efficiently encode each of the projected object proposals $\\mathcal{O}$ (bounding boxes of proposals projected on to each of the images with known poses $\\xi$) using VLAD with FLAIR, to obtain $\\Psi$. The resulting feature vector $\\Psi$ is used to train and predict likelihood of target label/category $p(x_i\\mid y)$ of the object contained in each of the object proposals. The likelihoods for each object $o \\in \\mathcal{O}$ are aggregated across each of the viewpoints $\\xi$ to obtain robust object category prediction.}     \\label{fig:recognition-pipeline} \\end{figure*}  We present several experimental results validating the improved object proposition and recognition performance of our proposed system: (i) The system is compared against the current state-of-the-art~\\cite{lai2012detection,laiunsupervised} on the UW-RGBD Scene~\\cite{lai2011large,laiunsupervised} Dataset. We compare the improved recognition performance of being SLAM-aware, to being SLAM-oblivious (ii) The multi-view object proposal method introduced is shown to outperform single-view object proposal strategies such as BING~\\cite{cheng2014bing} on the UW-RGBD dataset, that provide object candidates solely on a single-view. (iii) The run-time performance of our system is analysed, with specific discussion on the scalability of our approach, compared to existing state-of-the-art methods~\\cite{lai2012detection, laiunsupervised}.         \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZz\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "related work\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZz\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "experiments\n",
      "near-constant time. Through experiments, we illustrate the utility of using such a system to effectively detect and recognize objects, incorporating multiple object viewpoint detections into a unified prediction hypothesis. The performance of the proposed recognition system is evaluated on the UW RGB-D Dataset, showing strong recognition performance and scalable run-time performance compared to current state-of-the-art recognition systems.  \\end{abstract}  \\IEEEpeerreviewmaketitle  % (v) We show via experiments that our system is able to roughly learn the         In this section, we evaluate the proposed SLAM-aware object recognition method. In our experiments, we extensively evaluate our SLAM-aware recognition system on the popular UW RGB-D Dataset (v2)\\cite{lai2011large,laiunsupervised}. We compare against the current state-of-the-art solution proposed by~\\citet{lai2012detection}, that utilize full map and camera location information for improved recognition performance. The UW RGB-D dataset contains a total 51 object categories, however, in order to maintain a fair comparison, we consider the same set of 5 objects as noted in~\\cite{lai2012detection}. In experiment 3, we propose scalable recognition solutions, increasing the number of objects considered to all 51 object categories in the UW RGB-D Dataset.\\vspace{1mm}   \\textbf{Experiment 1: SLAM-Aware Object Recognition Performance Evaluation} \\label{experiment-1} We train and evaluate our system on the UW RGB-D Scene Dataset~\\cite{lai2011large,laiunsupervised}, providing mean-Average Precision (mAP) estimates (see Table~\\ref{tab:mAP-SLAM}) for the object recognition task and compare against existing methods~\\cite{lai2012detection}. We split our    \\input{results/mAP-slam}  \\textit{(i) Single-View recognition performance: } First, we evaluate the recognition performance of our proposed system on each of the scenes in the UW-RGB-D Scene Dataset on a per-frame basis, detecting and classifying objects that occur every 5 frames in each scene (as done in~\\cite{lai2012detection}). Each object category is trained from images in the Object Dataset, that includes several viewpoints of object instances with their corresponding mask, and category information. Using training parameters identical to the previous experiment, we achieve a performance of 81.5 mAP as compared to the detector performance of 61.7 mAP reported in~\\cite{lai2012detection}. Recognition is done on a per-image basis, and averaged across all test images for reporting. Figure~\\ref{fig:frame-detection-results} shows the recognition results of our system on a per-frame basis. We ignore regions labeled as background in the figure for clarity and only report the correct and incorrect predictions in green and red respectively. \\vspace{1mm}  \\textit{(ii) Multi-View recognition performance: } In this section, we investigate the performance of a SLAM-aware object recognition system. We compare this to a SLAM-oblivious object detector described previously, and evaluate using ground truth provided. Using the poses $\\xi$ and reconstructed map $\\mathcal{M}$, multi-view object candidates are proposed and projected onto each of the images for each scene sequence. Using the candidates provided as input to the recognition system, the system predicts the likelihood and corresponding category of an object (including background) contained in a candidate bounding box. For each of the objects $o \\in \\mathcal{O}$ proposed, the summed log-likelihood is computed (as in Eqn.~\\ref{eq:multi-view-evidence}) to estimate the most likely object category over all the images for a particular scene sequence. We achieve 89.8 mAP recognition performance on the 5 objects in each of the scenes in~\\cite{laiunsupervised} that was successfully reconstructed by the ORB-SLAM-based semi-dense mapping system. Figures~\\ref{fig:intro-fig},~\\ref{fig:multi-view-objectness}, ~\\ref{fig:slam-detection-results-1} and~\\ref{fig:more-slam-detect} illustrate the capabilities of the proposed system in providing robust recognition performance by taking advantage of the monocular visual SLAM-backend. Figure~\\ref{fig:mAP-SLAM} illustrates the average precision-recall performance on the UW RGB-D dataset, comparing the classical frame-based and our SLAM-aware approach. As expected, with additional object viewpoints, our proposed SLAM-aware solution predicts with improved precision and recall. In comparison to that of HMP2D+3D~\\cite{laiunsupervised}, they achieve only slightly higher overall recognition performance of 90.9 mAP, as their recognition pipeline takes advantage of the RGB and depth input to improve overall scene reconstruction. We do note that while we perform comparably with HMP2D+3D~\\cite{laiunsupervised}, our BoVW+FLAIR architecture allows our system to scale to a large number of object categories with \\textit{near-constant run-time}. We investigate the run-time performance and scalability concerns further in Experiment 3.  \\vspace{1mm}        \\begin{figure}[b]   \\centering   \\vspace{2mm}     \\includegraphics[width=0.95\\columnwidth]{graphics/objectness/objectness.pdf}      \\caption{\\textit{Varying number of proposals:} We experiment with varied       number of bounding boxes for the BING object proposal method, and compare       against our multi-view object proposal method that uses considerably fewer       number of bounding boxes to get similar or better recall rates. The numbers next       to the label indicate the average number of windows proposed in the image. }     \\label{fig:objectness-performance} \\end{figure}  \\textbf{Experiment 2: Multi-View Objectness} \\label{subsec:slam-driven-objectness} In this experiment, we investigate the effectiveness of our multi-view object proposal method in identifying category-independent objects in a continuous video stream. We compare the recall of our object proposal method with the recently introduced BING~\\cite{cheng2014bing} object proposal technique, whose performance in detection rate (DR) and run-time claim to be promising. We compare against the BING method, varying the number of proposed object candidates by picking proposals in descending order of their objectness score. Figure~\\ref{fig:objectness-performance} compares the overall performance of our multi-view object proposal method that achieves better recall rates, for a particular IoU threshold with considerably fewer object proposals. The results provided are evaluated on all the scenes provided in the UW-RGB-D dataset (v2)~\\cite{laiunsupervised}.     \\textbf{Experiment 3: Scalable recognition and run-time evaluation} \\label{subsec:results-run-time-perf} In this section, we investigate the run-time performance of computing VLAD with integral histograms (FLAIR) for our system and compare against previously proposed approaches~\\cite{van2014fisher,lai2012detection}. We measure the average speed for feature-extraction (Dense-SIFT) and feature-encoding (VLAD) as they take up over 95\\% of the overall compute time. All experiments were conducted with a single-thread on an Intel Core-i7-3920XM (2.9GHz).      \\input{results/runtime.tex}  \\citet{van2014fisher} reports that the overall feature extraction and encoding takes 5.15s (VQ 0.55s, FLAIR construction 0.6s, VLAD+FLAIR 4.0s) per image, with the following parameters (2px step size, 3 Pyr. Scales, [1x1],~[4x4] spatial pyramid bins). With significantly fewer candidate proposals, and careful implementation, our system is able to achieve the same (with 4px step size) in approximately 1.6s. With reference to~\\cite{lai2012detection}, where the run-time performance of the sliding-window approach is directly proportional to the number of object categories detectable, the authors report an overall run-time of 1.8s for 5 object categories. However, scaling up their detection to larger number of objects would imply costly runtimes, making it highly impractical for real-time purposes. The run-time of our approach (based on~\\cite{van2014fisher}), on the other hand, is scalable to a larger number of object categories, making it a strong contender for real-time recognition systems. We summarize the run-times of our approach compared to that of~\\cite{lai2012detection} and~\\cite{laiunsupervised} in Table~\\ref{table:runtime}. \\vspace{1mm}                  \\begin{figure}[h]   \\centering     \\includegraphics[width=\\columnwidth]{graphics/slam_detect/slam_detect-4.png}      \\caption{More illustrations of the superior performance of the         SLAM-aware object recognition in scenarios of ambiguity and         occlusions. The coffee mug is misidentified as a soda can, and the cap         in the bottom row is occluded by the cereal box.}     \\label{fig:more-slam-detect} \\end{figure}  \\textbf{Discussion and Future Work} While there are benefits to running a monocular visual SLAM-backend for recognition purposes, the inter-dependence of the recognition system on this backend makes it vulnerable to the same robustness concerns that pertain to monocular visual SLAM. In our experiments, we noticed inadequacies in the semi-dense vSLAM implementation that failed to reconstruct the scene on few occasions. To further emphasize recognition scalability, we are actively collecting a larger scaled dataset (in increased map area, and number of objects) to show the extent of capabilities of the proposed system. Furthermore, we realize the importance of real-time capabilities of such recognition systems, and intend to generalize the architecture to a streaming approach in the near future. We also hope to release the source code for our proposed method, allowing scalable and customizable training with fast run-time performance during live operation.    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZz\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "monocular slam supported \\\\object recognition\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZz\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "acknowledgments\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZz\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "conclusion\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill in each of the sections \n",
    "for name in names:\n",
    "    print '\\n\\n\\n\\n\\n\\ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZz\\n\\n\\n\\n\\n\\n'\n",
    "    print name\n",
    "    flag = 0\n",
    "    \n",
    "    temp = []\n",
    "    for line in content:\n",
    "    \n",
    "        # fill in sections\n",
    "        val =  line.find(name)\n",
    "#         print val\n",
    "        if val != -1:\n",
    "            flag = 1\n",
    "            val2 = line.find('}')\n",
    "            temp.append(line[(val2+1):])\n",
    "#             print temp\n",
    "            continue\n",
    "        val3 =  line.find('\\section')\n",
    "        if val3 != -1:\n",
    "            flag = 0\n",
    "        if flag == 1:\n",
    "            if line != '':\n",
    "                if line[0] != '%':\n",
    "                    temp.append(line)\n",
    "#     print ''.join(temp)\n",
    "    names[name] = ''.join(temp)\n",
    "    print names[name]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In this work, we develop a monocular SLAM-aware object recognition system that is able to achieve considerably stronger recognition performance, as compared to classical object recognition systems that function on a frame-by-frame basis. By incorporating several key ideas including multi-view object proposals and efficient feature encoding methods, our proposed system is able to detect and robustly recognize objects in its environment using a single RGB camera in near-constant time. Through experiments, we illustrate the utility of using such a system to effectively detect and recognize objects, incorporating multiple object viewpoint detections into a unified prediction hypothesis. The performance of the proposed recognition system is evaluated on the UW RGB-D Dataset, showing strong recognition performance and scalable run-time performance compared to current state-of-the-art recognition systems.  \n"
     ]
    }
   ],
   "source": [
    "# fill in the abstract\n",
    "flag = 0\n",
    "temp = []\n",
    "for line in content:\n",
    "    val = line.find('n{abstract}')\n",
    "    if val != -1:\n",
    "        flag = 1\n",
    "        val2 = line.find('}')\n",
    "        temp.append(line[(val2+1):])\n",
    "        continue\n",
    "    val3 =  line.find('d{abstract}')\n",
    "    if val3 != -1:\n",
    "        flag = 0\n",
    "    if flag == 1:\n",
    "        if line != '':\n",
    "            if line[0] != '%':\n",
    "                temp.append(line)\n",
    "# print ''.join(temp)\n",
    "names['abstract'] = ''.join(temp)\n",
    "print names['abstract']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\PAPERTITLEFMT{}\n"
     ]
    }
   ],
   "source": [
    "# fill in the title\n",
    "flag = 0\n",
    "temp = []\n",
    "for line in content:\n",
    "    val = line.find('title{')\n",
    "    if val != -1:\n",
    "#         print line\n",
    "        flag = 1\n",
    "        val2 = line.find('}')\n",
    "        temp.append(line[val+6:(val2+1)])\n",
    "#         continue\n",
    "#     val3 =  line.find('d{abstract}')\n",
    "#     if val3 != -1:\n",
    "#         flag = 0\n",
    "#     if flag == 1:\n",
    "#         if line != '':\n",
    "#             if line[0] != '%':\n",
    "#                 temp.append(line)\n",
    "names['title'] = ''.join(temp)\n",
    "print names['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slam-aware object recognition\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "introduction\n",
      " Object recognition is a vital component in a robot's repertoire of skills. Traditional object recognition methods have focused on improving recognition performance (Precision-Recall, or mean Average-Precision) on specific datasets~\\cite{everingham2010pascal, ILSVRC15}. While these datasets provide sufficient variability in object categories and instances, the training data mostly consists of images of arbitrarily picked scenes and/or objects. Robots, on the other hand, perceive their environment as a continuous image stream, observing the same object several times, and from multiple viewpoints, as it constantly moves around in its immediate environment. As a result, object detection and recognition can be further bolstered if the robot were capable of simultaneously localizing itself and mapping (SLAM) its immediate environment - by integrating object detection evidences across multiple views.       \\begin{figure}[!t]   \\centering   \\begin{tabular}{c}     \\includegraphics[width=\\columnwidth]{graphics/slam_detect/intro.pdf}   \\end{tabular}   \\caption{The proposed SLAM-aware object recognition system is able to robustly     localize and recognize several objects in the scene, aggregating detection evidence across multiple views. Annotations in white are provided for clarity and are actual predictions     proposed by our system.}   \\label{fig:intro-fig} \\end{figure}     We refer to a ``SLAM-aware'' system as - one that has access to the map of its observable surroundings as it builds it incrementally and the location of its camera at any point in time. This is in contrast to classical recognition systems that are ``SLAM-oblivious'' - those that detect and recognize objects on a frame-by-frame basis without being cognizant of the map of its environment, the location of its camera, or that objects may be situated within these maps. \\textit{In this paper, we develop the ability for a SLAM-aware system to robustly recognize objects in its environment, using an RGB camera as its only sensory input} (Figure~\\ref{fig:intro-fig}).  We make the following contributions towards this end: Using state-of-the-art semi-dense map reconstruction techniques in monocular visual SLAM as pre-processed input, we introduce the capability to propose multi-view consistent object candidates, as the camera observes instances of objects across several disparate viewpoints. Leveraging this object proposal method, we incorporate some of the recent advancements in bag-of-visual-words-based (BoVW) object classification~\\cite{jegou2010aggregating,delhumeau2013revisiting,arandjelovic2013all} and efficient box-encoding methods~\\cite{van2014fisher} to enable strong recognition performance.  The integration of this system with a monocular visual-SLAM (vSLAM) back-end also enables us to take advantage of both the reconstructed map and camera location to significantly bolster recognition performance. Additionally, our system design allows the run-time performance to be scalable to a larger number of object categories, with near-constant run-time for most practical object recognition tasks.   \\begin{figure*}[!t]     \\centering     % \\def\\svgwidth{\\columnwidth}     % \\input{graphics/slam_detect/pipeline.pdf_tex}     \\includegraphics[width=2\\columnwidth]{graphics/slam_detect/pipeline.pdf}     \\caption{Outline of the SLAM-aware object recognition pipeline. Given an input RGB image stream $\\mathcal{I}$, we first reconstruct the scene in a semi-dense fashion using an existing monocular visual-SLAM implementation (ORB-SLAM) with a semi-dense depth estimator, and subsequently extract relevant map $\\mathcal{M}$, keyframe $\\mathcal{K}$ and pose information $\\xi$. We perform multi-scale density-based segmentation on the reconstructed scene to obtain object proposals $\\mathcal{O}$ that are consistent across multiple views. On each of the images in the input RGB image stream $\\mathcal{I}$, we compute Dense-SIFT ($\\mathbb{R}^{128}$) + RGB ($\\mathbb{R}^{3}$) and reduce it to $\\Phi \\in \\mathbb{R}^{80}$ via PCA. The features $\\Phi$ are then used to efficiently encode each of the projected object proposals $\\mathcal{O}$ (bounding boxes of proposals projected on to each of the images with known poses $\\xi$) using VLAD with FLAIR, to obtain $\\Psi$. The resulting feature vector $\\Psi$ is used to train and predict likelihood of target label/category $p(x_i\\mid y)$ of the object contained in each of the object proposals. The likelihoods for each object $o \\in \\mathcal{O}$ are aggregated across each of the viewpoints $\\xi$ to obtain robust object category prediction.}     \\label{fig:recognition-pipeline} \\end{figure*}  We present several experimental results validating the improved object proposition and recognition performance of our proposed system: (i) The system is compared against the current state-of-the-art~\\cite{lai2012detection,laiunsupervised} on the UW-RGBD Scene~\\cite{lai2011large,laiunsupervised} Dataset. We compare the improved recognition performance of being SLAM-aware, to being SLAM-oblivious (ii) The multi-view object proposal method introduced is shown to outperform single-view object proposal strategies such as BING~\\cite{cheng2014bing} on the UW-RGBD dataset, that provide object candidates solely on a single-view. (iii) The run-time performance of our system is analysed, with specific discussion on the scalability of our approach, compared to existing state-of-the-art methods~\\cite{lai2012detection, laiunsupervised}.         \n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "abstract\n",
      " In this work, we develop a monocular SLAM-aware object recognition system that is able to achieve considerably stronger recognition performance, as compared to classical object recognition systems that function on a frame-by-frame basis. By incorporating several key ideas including multi-view object proposals and efficient feature encoding methods, our proposed system is able to detect and robustly recognize objects in its environment using a single RGB camera in near-constant time. Through experiments, we illustrate the utility of using such a system to effectively detect and recognize objects, incorporating multiple object viewpoint detections into a unified prediction hypothesis. The performance of the proposed recognition system is evaluated on the UW RGB-D Dataset, showing strong recognition performance and scalable run-time performance compared to current state-of-the-art recognition systems.  \n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "title\n",
      "\\PAPERTITLEFMT{}\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "related work\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "experiments\n",
      "near-constant time. Through experiments, we illustrate the utility of using such a system to effectively detect and recognize objects, incorporating multiple object viewpoint detections into a unified prediction hypothesis. The performance of the proposed recognition system is evaluated on the UW RGB-D Dataset, showing strong recognition performance and scalable run-time performance compared to current state-of-the-art recognition systems.  \\end{abstract}  \\IEEEpeerreviewmaketitle  % (v) We show via experiments that our system is able to roughly learn the         In this section, we evaluate the proposed SLAM-aware object recognition method. In our experiments, we extensively evaluate our SLAM-aware recognition system on the popular UW RGB-D Dataset (v2)\\cite{lai2011large,laiunsupervised}. We compare against the current state-of-the-art solution proposed by~\\citet{lai2012detection}, that utilize full map and camera location information for improved recognition performance. The UW RGB-D dataset contains a total 51 object categories, however, in order to maintain a fair comparison, we consider the same set of 5 objects as noted in~\\cite{lai2012detection}. In experiment 3, we propose scalable recognition solutions, increasing the number of objects considered to all 51 object categories in the UW RGB-D Dataset.\\vspace{1mm}   \\textbf{Experiment 1: SLAM-Aware Object Recognition Performance Evaluation} \\label{experiment-1} We train and evaluate our system on the UW RGB-D Scene Dataset~\\cite{lai2011large,laiunsupervised}, providing mean-Average Precision (mAP) estimates (see Table~\\ref{tab:mAP-SLAM}) for the object recognition task and compare against existing methods~\\cite{lai2012detection}. We split our    \\input{results/mAP-slam}  \\textit{(i) Single-View recognition performance: } First, we evaluate the recognition performance of our proposed system on each of the scenes in the UW-RGB-D Scene Dataset on a per-frame basis, detecting and classifying objects that occur every 5 frames in each scene (as done in~\\cite{lai2012detection}). Each object category is trained from images in the Object Dataset, that includes several viewpoints of object instances with their corresponding mask, and category information. Using training parameters identical to the previous experiment, we achieve a performance of 81.5 mAP as compared to the detector performance of 61.7 mAP reported in~\\cite{lai2012detection}. Recognition is done on a per-image basis, and averaged across all test images for reporting. Figure~\\ref{fig:frame-detection-results} shows the recognition results of our system on a per-frame basis. We ignore regions labeled as background in the figure for clarity and only report the correct and incorrect predictions in green and red respectively. \\vspace{1mm}  \\textit{(ii) Multi-View recognition performance: } In this section, we investigate the performance of a SLAM-aware object recognition system. We compare this to a SLAM-oblivious object detector described previously, and evaluate using ground truth provided. Using the poses $\\xi$ and reconstructed map $\\mathcal{M}$, multi-view object candidates are proposed and projected onto each of the images for each scene sequence. Using the candidates provided as input to the recognition system, the system predicts the likelihood and corresponding category of an object (including background) contained in a candidate bounding box. For each of the objects $o \\in \\mathcal{O}$ proposed, the summed log-likelihood is computed (as in Eqn.~\\ref{eq:multi-view-evidence}) to estimate the most likely object category over all the images for a particular scene sequence. We achieve 89.8 mAP recognition performance on the 5 objects in each of the scenes in~\\cite{laiunsupervised} that was successfully reconstructed by the ORB-SLAM-based semi-dense mapping system. Figures~\\ref{fig:intro-fig},~\\ref{fig:multi-view-objectness}, ~\\ref{fig:slam-detection-results-1} and~\\ref{fig:more-slam-detect} illustrate the capabilities of the proposed system in providing robust recognition performance by taking advantage of the monocular visual SLAM-backend. Figure~\\ref{fig:mAP-SLAM} illustrates the average precision-recall performance on the UW RGB-D dataset, comparing the classical frame-based and our SLAM-aware approach. As expected, with additional object viewpoints, our proposed SLAM-aware solution predicts with improved precision and recall. In comparison to that of HMP2D+3D~\\cite{laiunsupervised}, they achieve only slightly higher overall recognition performance of 90.9 mAP, as their recognition pipeline takes advantage of the RGB and depth input to improve overall scene reconstruction. We do note that while we perform comparably with HMP2D+3D~\\cite{laiunsupervised}, our BoVW+FLAIR architecture allows our system to scale to a large number of object categories with \\textit{near-constant run-time}. We investigate the run-time performance and scalability concerns further in Experiment 3.  \\vspace{1mm}        \\begin{figure}[b]   \\centering   \\vspace{2mm}     \\includegraphics[width=0.95\\columnwidth]{graphics/objectness/objectness.pdf}      \\caption{\\textit{Varying number of proposals:} We experiment with varied       number of bounding boxes for the BING object proposal method, and compare       against our multi-view object proposal method that uses considerably fewer       number of bounding boxes to get similar or better recall rates. The numbers next       to the label indicate the average number of windows proposed in the image. }     \\label{fig:objectness-performance} \\end{figure}  \\textbf{Experiment 2: Multi-View Objectness} \\label{subsec:slam-driven-objectness} In this experiment, we investigate the effectiveness of our multi-view object proposal method in identifying category-independent objects in a continuous video stream. We compare the recall of our object proposal method with the recently introduced BING~\\cite{cheng2014bing} object proposal technique, whose performance in detection rate (DR) and run-time claim to be promising. We compare against the BING method, varying the number of proposed object candidates by picking proposals in descending order of their objectness score. Figure~\\ref{fig:objectness-performance} compares the overall performance of our multi-view object proposal method that achieves better recall rates, for a particular IoU threshold with considerably fewer object proposals. The results provided are evaluated on all the scenes provided in the UW-RGB-D dataset (v2)~\\cite{laiunsupervised}.     \\textbf{Experiment 3: Scalable recognition and run-time evaluation} \\label{subsec:results-run-time-perf} In this section, we investigate the run-time performance of computing VLAD with integral histograms (FLAIR) for our system and compare against previously proposed approaches~\\cite{van2014fisher,lai2012detection}. We measure the average speed for feature-extraction (Dense-SIFT) and feature-encoding (VLAD) as they take up over 95\\% of the overall compute time. All experiments were conducted with a single-thread on an Intel Core-i7-3920XM (2.9GHz).      \\input{results/runtime.tex}  \\citet{van2014fisher} reports that the overall feature extraction and encoding takes 5.15s (VQ 0.55s, FLAIR construction 0.6s, VLAD+FLAIR 4.0s) per image, with the following parameters (2px step size, 3 Pyr. Scales, [1x1],~[4x4] spatial pyramid bins). With significantly fewer candidate proposals, and careful implementation, our system is able to achieve the same (with 4px step size) in approximately 1.6s. With reference to~\\cite{lai2012detection}, where the run-time performance of the sliding-window approach is directly proportional to the number of object categories detectable, the authors report an overall run-time of 1.8s for 5 object categories. However, scaling up their detection to larger number of objects would imply costly runtimes, making it highly impractical for real-time purposes. The run-time of our approach (based on~\\cite{van2014fisher}), on the other hand, is scalable to a larger number of object categories, making it a strong contender for real-time recognition systems. We summarize the run-times of our approach compared to that of~\\cite{lai2012detection} and~\\cite{laiunsupervised} in Table~\\ref{table:runtime}. \\vspace{1mm}                  \\begin{figure}[h]   \\centering     \\includegraphics[width=\\columnwidth]{graphics/slam_detect/slam_detect-4.png}      \\caption{More illustrations of the superior performance of the         SLAM-aware object recognition in scenarios of ambiguity and         occlusions. The coffee mug is misidentified as a soda can, and the cap         in the bottom row is occluded by the cereal box.}     \\label{fig:more-slam-detect} \\end{figure}  \\textbf{Discussion and Future Work} While there are benefits to running a monocular visual SLAM-backend for recognition purposes, the inter-dependence of the recognition system on this backend makes it vulnerable to the same robustness concerns that pertain to monocular visual SLAM. In our experiments, we noticed inadequacies in the semi-dense vSLAM implementation that failed to reconstruct the scene on few occasions. To further emphasize recognition scalability, we are actively collecting a larger scaled dataset (in increased map area, and number of objects) to show the extent of capabilities of the proposed system. Furthermore, we realize the importance of real-time capabilities of such recognition systems, and intend to generalize the architecture to a streaming approach in the near future. We also hope to release the source code for our proposed method, allowing scalable and customizable training with fast run-time performance during live operation.    \n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "monocular slam supported \\\\object recognition\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "acknowledgments\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "conclusion\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name in names:\n",
    "    print name\n",
    "    print names[name]\n",
    "    print '\\n-----------------------------------------------------\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syn_relWork = ['related work', 'background']\n",
    "syn_conc = ['conclusion', 'future work']\n",
    "#TODO: fix the discussion stuff because not all fit in here that should?!!?!\n",
    "syn_disc = ['discussion', 'results']\n",
    "#TODO: Add other sections to ignore such as acknowledgements and appendix and such!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slam-aware object recognition\n",
      "abstract\n",
      "title\n",
      "experiments\n",
      "monocular slam supported \\\\object recognition\n",
      "acknowledgments\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cats = {'title':'','abstract':'','introduction':'','related work':'','methodology':'','discussion':'','conclusion':''}\n",
    "\n",
    "# title\n",
    "cats['title'] = names['title']\n",
    "    \n",
    "# abstract\n",
    "cats['abstract'] = names['abstract']\n",
    "\n",
    "\n",
    "# TODO: This will overwrite what already exists in the dict, not append to it!!!!\n",
    "# other sections\n",
    "for name in names:\n",
    "#     print name.lower()\n",
    "    \n",
    "    # introduction\n",
    "    if name.lower() == 'introduction':\n",
    "        cats['introduction'] = names[name]\n",
    "    \n",
    "    # related work\n",
    "    flag_rw = 0\n",
    "    for nm in syn_relWork:\n",
    "        val = (name.lower()).find(nm)\n",
    "        if val != -1:\n",
    "            flag_rw = 1\n",
    "    if flag_rw == 1:\n",
    "#     if name.lower() == 'related work' or name.lower() == 'background':\n",
    "        cats['related work'] = cats['related work'].join(names[name])    \n",
    "       \n",
    "    \n",
    "    # conclusion\n",
    "    flag_c = 0\n",
    "    for nm in syn_conc:\n",
    "        val = (name.lower()).find(nm)\n",
    "        if val != -1:\n",
    "            flag_c = 1\n",
    "    if flag_c == 1:\n",
    "#     if name.lower() == 'conclusion' or name.lower() == 'future work':\n",
    "        cats['conclusion'] = cats['conclusion'].join(names[name])\n",
    "    \n",
    "    \n",
    "    # methodology\n",
    "#     all other sections should go here!\n",
    "# TODO: Need to get rid of ignore sections like acknowledgements and such!!!!\n",
    "    if name.lower() != 'introduction' and flag_rw == 0 and flag_c == 0:\n",
    "\n",
    "        print name.lower()\n",
    "        cats['methodology'] = cats['methodology'].join(names[name])\n",
    "        \n",
    "    \n",
    "    \n",
    "    # TODO: What about discussion/results sections?!!?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n",
      "\\PAPERTITLEFMT{}\n",
      "\n",
      "----------------\n",
      "\n",
      "introduction\n",
      " Object recognition is a vital component in a robot's repertoire of skills. Traditional object recognition methods have focused on improving recognition performance (Precision-Recall, or mean Average-Precision) on specific datasets~\\cite{everingham2010pascal, ILSVRC15}. While these datasets provide sufficient variability in object categories and instances, the training data mostly consists of images of arbitrarily picked scenes and/or objects. Robots, on the other hand, perceive their environment as a continuous image stream, observing the same object several times, and from multiple viewpoints, as it constantly moves around in its immediate environment. As a result, object detection and recognition can be further bolstered if the robot were capable of simultaneously localizing itself and mapping (SLAM) its immediate environment - by integrating object detection evidences across multiple views.       \\begin{figure}[!t]   \\centering   \\begin{tabular}{c}     \\includegraphics[width=\\columnwidth]{graphics/slam_detect/intro.pdf}   \\end{tabular}   \\caption{The proposed SLAM-aware object recognition system is able to robustly     localize and recognize several objects in the scene, aggregating detection evidence across multiple views. Annotations in white are provided for clarity and are actual predictions     proposed by our system.}   \\label{fig:intro-fig} \\end{figure}     We refer to a ``SLAM-aware'' system as - one that has access to the map of its observable surroundings as it builds it incrementally and the location of its camera at any point in time. This is in contrast to classical recognition systems that are ``SLAM-oblivious'' - those that detect and recognize objects on a frame-by-frame basis without being cognizant of the map of its environment, the location of its camera, or that objects may be situated within these maps. \\textit{In this paper, we develop the ability for a SLAM-aware system to robustly recognize objects in its environment, using an RGB camera as its only sensory input} (Figure~\\ref{fig:intro-fig}).  We make the following contributions towards this end: Using state-of-the-art semi-dense map reconstruction techniques in monocular visual SLAM as pre-processed input, we introduce the capability to propose multi-view consistent object candidates, as the camera observes instances of objects across several disparate viewpoints. Leveraging this object proposal method, we incorporate some of the recent advancements in bag-of-visual-words-based (BoVW) object classification~\\cite{jegou2010aggregating,delhumeau2013revisiting,arandjelovic2013all} and efficient box-encoding methods~\\cite{van2014fisher} to enable strong recognition performance.  The integration of this system with a monocular visual-SLAM (vSLAM) back-end also enables us to take advantage of both the reconstructed map and camera location to significantly bolster recognition performance. Additionally, our system design allows the run-time performance to be scalable to a larger number of object categories, with near-constant run-time for most practical object recognition tasks.   \\begin{figure*}[!t]     \\centering     % \\def\\svgwidth{\\columnwidth}     % \\input{graphics/slam_detect/pipeline.pdf_tex}     \\includegraphics[width=2\\columnwidth]{graphics/slam_detect/pipeline.pdf}     \\caption{Outline of the SLAM-aware object recognition pipeline. Given an input RGB image stream $\\mathcal{I}$, we first reconstruct the scene in a semi-dense fashion using an existing monocular visual-SLAM implementation (ORB-SLAM) with a semi-dense depth estimator, and subsequently extract relevant map $\\mathcal{M}$, keyframe $\\mathcal{K}$ and pose information $\\xi$. We perform multi-scale density-based segmentation on the reconstructed scene to obtain object proposals $\\mathcal{O}$ that are consistent across multiple views. On each of the images in the input RGB image stream $\\mathcal{I}$, we compute Dense-SIFT ($\\mathbb{R}^{128}$) + RGB ($\\mathbb{R}^{3}$) and reduce it to $\\Phi \\in \\mathbb{R}^{80}$ via PCA. The features $\\Phi$ are then used to efficiently encode each of the projected object proposals $\\mathcal{O}$ (bounding boxes of proposals projected on to each of the images with known poses $\\xi$) using VLAD with FLAIR, to obtain $\\Psi$. The resulting feature vector $\\Psi$ is used to train and predict likelihood of target label/category $p(x_i\\mid y)$ of the object contained in each of the object proposals. The likelihoods for each object $o \\in \\mathcal{O}$ are aggregated across each of the viewpoints $\\xi$ to obtain robust object category prediction.}     \\label{fig:recognition-pipeline} \\end{figure*}  We present several experimental results validating the improved object proposition and recognition performance of our proposed system: (i) The system is compared against the current state-of-the-art~\\cite{lai2012detection,laiunsupervised} on the UW-RGBD Scene~\\cite{lai2011large,laiunsupervised} Dataset. We compare the improved recognition performance of being SLAM-aware, to being SLAM-oblivious (ii) The multi-view object proposal method introduced is shown to outperform single-view object proposal strategies such as BING~\\cite{cheng2014bing} on the UW-RGBD dataset, that provide object candidates solely on a single-view. (iii) The run-time performance of our system is analysed, with specific discussion on the scalability of our approach, compared to existing state-of-the-art methods~\\cite{lai2012detection, laiunsupervised}.         \n",
      "\n",
      "----------------\n",
      "\n",
      "abstract\n",
      " In this work, we develop a monocular SLAM-aware object recognition system that is able to achieve considerably stronger recognition performance, as compared to classical object recognition systems that function on a frame-by-frame basis. By incorporating several key ideas including multi-view object proposals and efficient feature encoding methods, our proposed system is able to detect and robustly recognize objects in its environment using a single RGB camera in near-constant time. Through experiments, we illustrate the utility of using such a system to effectively detect and recognize objects, incorporating multiple object viewpoint detections into a unified prediction hypothesis. The performance of the proposed recognition system is evaluated on the UW RGB-D Dataset, showing strong recognition performance and scalable run-time performance compared to current state-of-the-art recognition systems.  \n",
      "\n",
      "----------------\n",
      "\n",
      "related work\n",
      "\n",
      "\n",
      "----------------\n",
      "\n",
      "methodology\n",
      "\n",
      "\n",
      "----------------\n",
      "\n",
      "discussion\n",
      "\n",
      "\n",
      "----------------\n",
      "\n",
      "conclusion\n",
      "\n",
      "\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cat in cats:\n",
    "    print cat\n",
    "    print cats[cat]\n",
    "    print '\\n----------------\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-d528c90d63ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfound\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mcats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'introduction'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'introduction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfound\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# TODO: The issue here is that order is not preserved, so if multiple things go in one category, the ordering is wrong so how much will this matter?!!?!?\n",
    "# --------------------------------------------\n",
    "\n",
    "\n",
    "# define the synonyms for categorization\n",
    "syn_intro = ['introduction']\n",
    "syn_relWork = ['related work', 'background']\n",
    "syn_meth = ['methodology', ] # TODO: Fix this, this should be all others\n",
    "syn_disc = ['discussion', 'results', 'experiment', 'implementation'] # TODOL What if dicussion and conclusion!?!?\n",
    "syn_conc = ['conclusion', 'future work', 'limitations']\n",
    "\n",
    "# loop through names and put into correct cat:\n",
    "for name in names:\n",
    "\n",
    "    # check title\n",
    "    if name == 'title':\n",
    "        cats['title'] = cats['title'].join(names[name])\n",
    "        \n",
    "    # check abstract\n",
    "    elif name == 'abstract':\n",
    "        cats['abstract'] = cats['abstract'].join(names[name])\n",
    "\n",
    "    # check other sections:\n",
    "    else:\n",
    "        \n",
    "        # set flag to see if name is found in anything but method syns\n",
    "        found = 0\n",
    "        \n",
    "        # check intro\n",
    "        for syn in syn_intro:\n",
    "            val = name.find(syn)\n",
    "            if val == -1:\n",
    "                found = found | 0\n",
    "            else:\n",
    "                cats['introduction'] = cats['introduction'].join(names[name])   \n",
    "                found = found | 1\n",
    "                \n",
    "        # check related work\n",
    "        for syn in syn_intro:\n",
    "            val = name.find(syn)\n",
    "            if val == -1:\n",
    "                found = found | 0\n",
    "            else:\n",
    "                cats['related work'] = cats['related work'].join(names[name])   \n",
    "                found = found | 1\n",
    "\n",
    "        # check discussion \n",
    "        for syn in syn_intro:\n",
    "            val = name.find(syn)\n",
    "            if val == -1:\n",
    "                found = found | 0\n",
    "            else:\n",
    "                cats['discussion'] = cats['discussion'].join(names[name])   \n",
    "                found = found | 1\n",
    "\n",
    "        # check conclusion\n",
    "        for syn in syn_intro:\n",
    "            val = name.find(syn)\n",
    "            if val == -1:\n",
    "                found = found | 0\n",
    "            else:\n",
    "                cats['conclusion'] = cats['conclusion'].join(names[name])   \n",
    "                found = found | 1\n",
    "    \n",
    "\n",
    "        # if not found, then put into methodology\n",
    "        if found == 0:\n",
    "            cats['methodology'] = cats['methodology'].join(names[name])   \n",
    "            \n",
    "    \n",
    "# # check intro\n",
    "# for syn in syn_intro:\n",
    "#     res = names.get(syn, -1)\n",
    "#     if res != -1:\n",
    "#     print res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
